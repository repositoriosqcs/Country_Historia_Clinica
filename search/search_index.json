{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introducci\u00f3n","text":""},{"location":"#introduccion","title":"Introducci\u00f3n","text":""},{"location":"#contexto","title":"Contexto","text":"<p>Durante el desarrollo del proyecto Dise\u00f1o Historia Cl\u00ednica - Gobierno de Datos, el equipo de QCS colabor\u00f3 estrechamente con el equipo de la Cl\u00ednica del Country en el dise\u00f1o funcional y estructural de su Historia Cl\u00ednica (HC). Este trabajo incluy\u00f3, a nivel funcional, la integraci\u00f3n de las aplicaciones involucradas y el desarrollo de funcionalidades que satisfacen las necesidades y requerimientos de la organizaci\u00f3n de manera eficiente, asegurando la alineaci\u00f3n con los objetivos estrat\u00e9gicos de la Cl\u00ednica.</p> <p>Para ello, el proyecto se estructur\u00f3 en cuatro grupos de trabajo, cada uno enfocado en aspectos clave para garantizar una implementaci\u00f3n exitosa.</p> <ul> <li>GRUPO 1: EXPLORACI\u00d3N JOURNEY PROCESOS      Analizar el ecosistema y entorno que rodea la Historia Cl\u00ednica (HC), tanto en procesos como en aplicaciones y datos, con la informaci\u00f3n entregada por el \u00e1rea de tecnolog\u00eda de Country.</li> <li>GRUPO 2: DISE\u00d1O INTEGRAL DE LA SOLUCI\u00d3N      Dise\u00f1o de la soluci\u00f3n para el planteamiento de la soluci\u00f3n desde todas las perspectivas necesarias. </li> <li>GRUPO 3: ASPECTOS FUNDAMENTALES      Definici\u00f3n Funcionalidades y Caracter\u00edsticas necesarias en t\u00e9rminos de rendimiento, fiabilidad, disponibilidad, acceso y dise\u00f1o.</li> <li>GRUPO 4: ESTRUCTURA DE DATOS      Modelo de datos centralizado con el an\u00e1lisis de la situaci\u00f3n actual de la informaci\u00f3n y recomendaciones de gobierno de datos. </li> </ul> <p>De esta manera, el presente documento se enfocada en el componente t\u00e9cnico del GRUPO 4: ESTRUCTURA DE DATOS</p>"},{"location":"#objetivo","title":"Objetivo:","text":"<p>El objetivo de este documento es presentar la documentaci\u00f3n t\u00e9cnica de los entregables 1 y 2 del GRUPO 4: ESTRUCTURA DE DATOS. Esto es, el Repositorio centralizado de Datos Param\u00e9tricos HC y el Modelo Anal\u00edtico An\u00e1lisis de Calidad de Datos Param\u00e9tricos Historia Cl\u00ednica.</p>"},{"location":"#entregable-1-repositorio-centralizado-de-datos-parametricos-hc","title":"Entregable 1: Repositorio centralizado de Datos Param\u00e9tricos HC","text":"<p>Corresponde a un Repositorio Centralizado que almacena los datos param\u00e9tricos de la historia cl\u00ednica, lo cual permite una gesti\u00f3n unificada y optimizada de la informaci\u00f3n. Para ello, se dise\u00f1aron e implementaron procesos de Extracci\u00f3n, Transformaci\u00f3n y Carga (ETL), lo cual sirve como insumo para realizar los an\u00e1lisis de calidad necesarios para mejorar el estado de los datos de a cara a la migraci\u00f3n a una nueva herramienta.</p> <p>Estos procesos ETL cumplen varias funciones clave:</p> <ul> <li> <p>Extracci\u00f3n: Captura los datos desde las bases COL_CTY y CTY_PRI, garantizando su disponibilidad en el repositorio centralizado.</p> </li> <li> <p>Transformaci\u00f3n: C\u00e1lculo de metadatos desde COL_CTY y CTY_PRI e indicadores de los componentes de calidad evaluados.</p> </li> <li> <p>Carga: Incorpora la informaci\u00f3n optimizada en el repositorio centralizado, facilitando su acceso y an\u00e1lisis.</p> </li> </ul>"},{"location":"#alcance","title":"Alcance","text":"<p>El Repositorio Centralizado cuenta con los siguientes elementos para su correcto funcionamiento:</p> <ul> <li>2 scripts Python para generaci\u00f3n y actualizaci\u00f3n autom\u00e1tica.</li> <li>Tabla maestra con lista de tablas a incluir, la cual se puede ajustar.</li> <li>Script SQL de creaci\u00f3n de repositorio generado autom\u00e1ticamente.</li> </ul>"},{"location":"#entregable-2-modelo-analitico-de-calidad-de-datos-parametricos-historia-clinica","title":"Entregable 2: Modelo Anal\u00edtico de Calidad de Datos Param\u00e9tricos Historia Cl\u00ednica","text":"<p>Se trata de un Tablero en Power BI dise\u00f1ado con el prop\u00f3sito de analizar y comprender el comportamiento de los datos, as\u00ed como evaluar el estado actual de la calidad de los datos param\u00e9tricos asociados a la Historia Cl\u00ednica.</p> <p>Este tablero permite a los usuarios visualizar informaci\u00f3n clave sobre la calidad de datos, a partir de 8 componentes:  Exactitud, Completitud, Consistencia, Actualizaci\u00f3n, Trazabilidad, Conformidad, Credibilidad y Compresibilidad. Lo cual permite identificar posibles inconsistencias, valores nulos o duplicados, y verificar el cumplimiento de las reglas de negocio establecidas en el manejo de la Historia Cl\u00ednica.</p> <p>Adem\u00e1s, incluye m\u00e9tricas que facilitan el seguimiento de la calidad de los datos en diferentes dimensiones, permitiendo detectar tendencias y patrones que puedan influir en la toma de decisiones estrat\u00e9gicas. Gracias a su capacidad de segmentaci\u00f3n y filtros din\u00e1micos, los usuarios pueden explorar los datos desde distintas perspectivas y obtener insights precisos sobre la informaci\u00f3n almacenada.</p> <p>Este tablero contribuye a fortalecer el gobierno de datos de la organizaci\u00f3n, optimizando los procesos de an\u00e1lisis y mejorando la confiabilidad de la informaci\u00f3n utilizada en la gesti\u00f3n de la historia cl\u00ednica.</p>"},{"location":"#alcance_1","title":"Alcance","text":"<ul> <li>3 scripts Python conectados al repositorio para actualizaci\u00f3n</li> <li>8 Indicadores de calidad de datos</li> <li>3 Visualizaciones de an\u00e1lisis</li> </ul>"},{"location":"buenaspracticas/","title":"Buenas Pr\u00e1cticas","text":""},{"location":"buenaspracticas/#buenas-practicas-implementadas","title":"Buenas Pr\u00e1cticas Implementadas","text":"<p>El Repositorio y Tablero desarrollados cuentan con una serie de buenas pr\u00e1cticas para garantizar eficiencia, escalabilidad y calidad. A continuaci\u00f3n, se destacan las principales pr\u00e1cticas aplicadas:</p>"},{"location":"buenaspracticas/#1-automatizacion-y-modularidad-en-el-etl","title":"1. Automatizaci\u00f3n y Modularidad en el ETL","text":"<p>Descripci\u00f3n:  El proyecto utiliza un flujo ETL modular y automatizado para facilitar la extracci\u00f3n, transformaci\u00f3n y carga de datos.</p> <p>Ejemplos:  - Uso de logs para registrar eventos claves y excepciones.  - Dise\u00f1o de funciones transversales y particulares para los procesos de ETLs </p> <p>Beneficios:  - Reutilizaci\u00f3n de c\u00f3digo. - Facilidad para realizar ajustes y escalabilidad.</p>"},{"location":"buenaspracticas/#2-gestion-de-credenciales-segura","title":"2. Gesti\u00f3n de Credenciales Segura","text":"<p>Descripci\u00f3n:  Las credenciales se gestionan de forma centralizada y segura mediante la definici\u00f3n de conexiones en funciones generales.</p> <p>Ejemplos:  - Las cadenas de conexi\u00f3n permiten configurarse f\u00e1cilmente para reapuntar las bases de origen y destino.  - Las cadenas de conexi\u00f3n se encuentran separadas de las ETLs principales por seguridad. </p> <p>Beneficios: - Mejora la seguridad al evitar exposici\u00f3n directa de credenciales en el c\u00f3digo.  - Facilita la configuraci\u00f3n de m\u00faltiples entornos. </p>"},{"location":"buenaspracticas/#3-registro-detallado-con-logging","title":"3. Registro Detallado con Logging","text":"<p>Descripci\u00f3n:  El uso del m\u00f3dulo <code>logging</code> asegura el registro detallado de eventos, errores y tiempos de procesamiento. </p> <p>Ejemplos: - Para cada ETL se crea autom\u00e1ticamente una carpeta donde se registran los logs del proceso.  - Se identifican los \u00e9xitos e inconsistencias que se pueden presentar durante la ejecuci\u00f3n de las ETLs. </p> <p>Beneficios:  - Facilita la depuraci\u00f3n y el monitoreo de procesos.  - Proporciona trazabilidad completa de las operaciones realizadas. </p>"},{"location":"buenaspracticas/#4-buenas-practicas-de-programacion-en-python","title":"4. Buenas Pr\u00e1cticas de Programaci\u00f3n en Python","text":"<p>Descripci\u00f3n:  El c\u00f3digo sigue las buenas pr\u00e1cticas de Python, como modularidad y manejo seguro de excepciones. </p> <p>Ejemplos:  - Manejo de excepciones para identificaci\u00f3n de posibles inconsistencias en la ejecuci\u00f3n del c\u00f3digo.  - Definici\u00f3n de funciones para contar con modularidad. </p> <p>Beneficios:  - Mejora la mantenibilidad del c\u00f3digo.  - Reduce errores inesperados durante la ejecuci\u00f3n. </p>"},{"location":"buenaspracticas/#conclusion","title":"Conclusi\u00f3n","text":"<p>Las pr\u00e1cticas implementadas en el proyecto garantizan la eficiencia y la escalabilidad t\u00e9cnica, y adem\u00e1s aseguran la calidad y la seguridad de los procesos. Este enfoque integral establece una base s\u00f3lida para la sostenibilidad de los entregables a lo largo del tiempo.</p>"},{"location":"conexion_BD/","title":"Conexi\u00f3n a Bases de Datos","text":""},{"location":"conexion_BD/#conexion-a-bases-de-datos","title":"Conexi\u00f3n a Bases de Datos","text":"<p>Las bases de datos definidas para el proyecto son COL_CTY y CTY_PRI a partir de las cuales se crea el repositorio centralizado con dos bases con sus respectivos datos param\u00e9tricos, las cuales son COL_CTY_PARAMETRICA y CTY_PRI_PARAMETRICA. Las conexiones se realizan por medio de python y se definen en el script funciones generales.</p>"},{"location":"conexion_BD/#cadena-de-conexion-a-bases-de-datos-de-origen","title":"Cadena de conexi\u00f3n a bases de datos de origen","text":"<p>Para conectarse a las base de origen COL_CTY y CTY_PRI se usan las siguientes cadenas de conexi\u00f3n en un diccionario: <pre><code># Diccionario de conexiones de origen\nconnections_start = {\n    \"CTY_PRI\": r\"DRIVER={SQL Server};SERVER=QCS-CONS04;DATABASE=CTY_PRI;Trusted_Connection=yes;\",\n    \"COL_CTY\": r\"DRIVER={SQL Server};SERVER=QCS-CONS04;DATABASE=COL_CTY;Trusted_Connection=yes;\",\n}\n</code></pre></p>"},{"location":"conexion_BD/#cadena-de-conexion-a-bases-de-datos-de-destino","title":"Cadena de conexi\u00f3n a bases de datos de destino","text":"<p>Para conectarse a las base de destino COL_CTY_PARAMETRICA y CTY_PRI_PARAMETRICA se usan las siguientes cadenas de conexi\u00f3n en un diccionario:</p> <pre><code># Diccionario de conexiones de destino\nconnections_final = {\n    \"CTY_PRI_PARAMETRICA\": r\"DRIVER={SQL Server};SERVER=QCS-CONS04;DATABASE=CTY_PRI_PARAMETRICA;Trusted_Connection=yes;\",\n    \"COL_CTY_PARAMETRICA\": r\"DRIVER={SQL Server};SERVER=QCS-CONS04;DATABASE=COL_CTY_PARAMETRICA;Trusted_Connection=yes;\",\n}\n</code></pre>"},{"location":"conexion_BD/#uso-de-cadenas-de-conexion","title":"Uso de cadenas de conexi\u00f3n","text":"<p>De esta manera, en las ETLs para conectarse a las bases de datos se usan estas cadenas de conexi\u00f3n. Por ejemplo, para conectarse a las base de origen se realiza con el siguiente c\u00f3digo: <pre><code>import pyodbc\nimport pandas as pd\nimport os\nimport time\nfrom funciones_generales import connections_start,connections_final,db_final,lista_tablas\n\n# Conectar a las bases y generar scripts\nesquema = 'dbo'\nfor db_name, conn_str in connections.items():\n   print(f\"Procesando base de datos: {db_name}\")\n   log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - Procesando base de datos: {db_name}\\n\")\n   try:\n      conn = pyodbc.connect(conn_str)\n      cursor = conn.cursor()\n   except Exception as e:\n      print(f\"Error procesando la base de datos {db_name}: {e}\")\n      log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - Error procesando la base de datos {db_name}: {e}\\n\")\n</code></pre> Estas conexiones ya est\u00e1n incluidas en las ETLS, por lo cual solo es necesario cambiar las cadenas de conexi\u00f3n para realizar la instalaci\u00f3n en el script de Funciones Generales</p>"},{"location":"conexion_BD/#notas-importantes","title":"Notas Importantes","text":"<p>Estas cadenas de conexi\u00f3n permiten reapuntar las bases de origen y destino de acuerdo a las necesidades de la organizaci\u00f3n. Es importante tener en cuenta los siguientes aspectos:      - Evite compartir las credenciales de acceso.      - Solo el personal autorizado debe tener acceso a las credenciales y ETLs </p>"},{"location":"instalacion/","title":"Instalaci\u00f3n y Configuraci\u00f3n","text":""},{"location":"instalacion/#instalacion-y-configuracion-del-entorno","title":"Instalaci\u00f3n y Configuraci\u00f3n del Entorno","text":"<p>Este apartado detalla los pasos necesarios para configurar un entorno funcional que permita la implementaci\u00f3n y ejecuci\u00f3n de los procesos de carga, transformaci\u00f3n y an\u00e1lisis de datos utilizados en el presente proyecto. El objetivo principal es garantizar una instalaci\u00f3n uniforme de herramientas y dependencias, configurar correctamente las conexiones a las bases de datos y estructurar el entorno para el \u00f3ptimo funcionamiento del Repositorio Centralizado y el Modelo Anal\u00edtico.</p>"},{"location":"instalacion/#pasos-para-la-configuracion-del-entorno","title":"Pasos para la Configuraci\u00f3n del Entorno","text":""},{"location":"instalacion/#1-instalacion-de-herramientas-necesarias","title":"1. Instalaci\u00f3n de Herramientas Necesarias","text":""},{"location":"instalacion/#versionamiento-de-herramientas","title":"Versionamiento de Herramientas","text":"<p>Las ETLs se desarrollaron en python, SQL server y Power Query de acuerdo a las definiciones realizadas entre los equipos en la mesa de trabajo de febrero de 2025</p>"},{"location":"instalacion/#11-sql-server","title":"1.1 SQL Server","text":"<p>Requisitos:</p> <ul> <li>Versi\u00f3n 2019</li> <li>Disponible a mayo de 2025 en el siguiente enlace: Descargar SQL Server 2019.</li> </ul>"},{"location":"instalacion/#12-sql-server-management-studio-ssms","title":"1.2 SQL Server Management Studio (SSMS)","text":"<p>Requisitos:</p> <ul> <li>Versi\u00f3n 20.1.10.0 o posterior.</li> <li>Disponible a mayo de 2025 en el siguiente enlace: Descargar SSMS.</li> </ul>"},{"location":"instalacion/#13-python","title":"1.3 Python","text":"<p>Requisitos:</p> <ul> <li>Versi\u00f3n 3.11.5 o posterior.</li> <li>En distribuciones Windows disponible en la tienda de Microsoft. Para otras distribuciones consultar el sitio oficial en el siguiente enlace: Consultar Sitio Oficial Python</li> </ul>"},{"location":"instalacion/#14-power-bi","title":"1.4 Power BI","text":"<p>Requisitos:</p> <ul> <li>Versi\u00f3n 2.139.2054.0 64-bit (enero de 2025)</li> <li>Se recomienda manejarlo en distribuciones Windows, para las cuales est\u00e1 disponible en la tienda de Microsoft.</li> </ul>"},{"location":"instalacion/#2-creacion-del-entorno-virtual","title":"2. Creaci\u00f3n del Entorno Virtual","text":"<p>Un entorno virtual permite organizar y aislar las dependencias necesarias para este proyecto. Siga estos pasos:</p> <p>1. Cree un entorno virtual ejecutando el siguiente comando:</p> <pre><code>python -m venv env\n</code></pre> <p>2. Active el entorno virtual:</p> <p>En Windows:</p> <pre><code>env\\Scripts\\activate\n</code></pre> <p>En Linux/macOS:</p> <pre><code>source env/bin/activate\n</code></pre>"},{"location":"instalacion/#3-instalacion-de-dependencias","title":"3. Instalaci\u00f3n de Dependencias","text":"<p>Una vez activado el entorno virtual, instale las librer\u00edas necesarias. Estas librer\u00edas junto con sus versiones se encuentran en el archivo requirements.txt disponible en la carpeta 01.Repositorio\\02.Scripts_Python.</p> <p></p> <p>Archivo requirements.txt en la carpeta carpeta 01.Repositorio\\02.Scripts_Python </p> <p> A continuaci\u00f3n, se presentan las librer\u00edas y las versiones indicadas en requirements.txt      - pandas==2.2.3      - numpy==2.2.4      - openpyxl==3.1.5  </p> <p>De esta manera, para instalar las librer\u00edas ejecutar el siguiente comando en la consola. En ambientes windows se recomienda usar PowerShell.</p> <p><pre><code>pip install -r requirements.txt\n</code></pre> En caso de error, identificar la librer\u00eda que presenta el error y correr el comando</p> <pre><code>pip install libreria\n</code></pre> <p>Luego ver la versi\u00f3n de la librer\u00eda con <pre><code>pip show libreria\n</code></pre> Actualizar la versi\u00f3n de la librer\u00eda en el archivo requirements.txt</p>"},{"location":"instalacion/#consideraciones-de-uso","title":"Consideraciones de uso","text":"<p>En esta secci\u00f3n se describen las condiciones y reglas de negocio m\u00ednimas requeridas para el adecuado funcionamiento del repositorio y modelo de anal\u00edtico.</p> <p>A.  Se debe establecer una ruta espec\u00edfica donde se dispongan los archivos que van a ser cargados a la herramienta, la cual cumpla con los niveles de seguridad adecuados de tal manera que solo el personal autorizado tenga acceso a esta.</p> <p>B.  La informaci\u00f3n que ingresa al modelo debe estar alojada en las rutas preestablecidas y consolidada en las estructuras propuestas, manteniendo la integridad de las variables.  Se debe tener en cuenta que no se recomienda hacer cambios a la estructura desarrollada.</p> <p>C.  La frecuencia de actualizaci\u00f3n est\u00e1 dada por las reglas de negocio definidas en el Documento de sugerencias en Gobierno de datos asociado con la calidad de los datos param\u00e9tricos de la historia cl\u00ednica, donde se sugiere una frecuencia mensual.</p>"},{"location":"instalacion/#conclusion","title":"Conclusi\u00f3n","text":"<p>Con estos pasos, el entorno estar\u00e1 listo para manejar los procesos ETL del proyecto. Al seguir estas indicaciones se asegura la configuraci\u00f3n correcta para ejecutar los scripts de Python.</p>"},{"location":"01.Repositorio/00.Inicio/","title":"Introducci\u00f3n","text":""},{"location":"01.Repositorio/00.Inicio/#01-repositorio-centralizado-de-datos-parametricos-hc","title":"01. Repositorio centralizado de Datos Param\u00e9tricos HC","text":"<p>El repositorio cuenta con tres partes para su ejecuci\u00f3n, las cuales se encuentran disponibles en los archivos de las siguientes carpetas:</p> <p></p>"},{"location":"01.Repositorio/00.Inicio/#indice","title":"\u00cdndice","text":"<ul> <li>01.Fuentes.  </li> <li>02.Scripts Python <ul> <li>Funciones Generales. </li> <li>01 Creaci\u00f3n Bases Repositorio. </li> <li>02 Poblar Bases Param\u00e9tricas Actualizaci\u00f3n. </li> </ul> </li> <li>03.Scripts SQL <ul> <li>01.COL_CTY_PARAMETRICA. </li> <li>01.CTY_PRI_PARAMETRICA. </li> </ul> </li> </ul>"},{"location":"01.Repositorio/01.Fuentes/","title":"1.1 Fuentes","text":""},{"location":"01.Repositorio/01.Fuentes/#11-fuentes","title":"1.1 Fuentes","text":"<p>En la carpeta 01.Repositorio/01.Fuentes se encuentra el archivo Excel 01.Tablas_Unicas.xlsx , que contiene el listado de tablas de las bases de datos COL_CTY y CTY_PRI. Este archivo incluye la descripci\u00f3n de cada tabla, informaci\u00f3n del grupo al que pertenece y una indicaci\u00f3n sobre si ha sido seleccionada para el repositorio. Este archivo permite seleccionar las tablas que van al repositorio y se recomienda no cambiar su configuraci\u00f3n. Cualquier cambio debe contar con la validaci\u00f3n del equipo de Country y una comprensi\u00f3n clara de los impactos en el repositorio y en el tablero. </p> <p></p> <p></p> <p>Vista del archivo 01.Tablas_Unicas.xlsx</p> <p></p>"},{"location":"01.Repositorio/Scripts_Python/01.Creacion_Bases_Repositorio/","title":"01 Creaci\u00f3n Bases Repositorio","text":""},{"location":"01.Repositorio/Scripts_Python/01.Creacion_Bases_Repositorio/#01-creacion-bases-repositorio","title":"01 Creaci\u00f3n Bases Repositorio","text":"<p>El objetivo de este script es generar el SQL que permite crear la base de datos param\u00e9trica. Es importante tener en cuenta que este script se debe ejecutar solo una vez para la instalaci\u00f3n y no es necesario para la actualizaci\u00f3n. En caso de ejecutarlo se debe tener en cuenta que se pierde la informaci\u00f3n hist\u00f3rica de los indicadores.</p>"},{"location":"01.Repositorio/Scripts_Python/01.Creacion_Bases_Repositorio/#librerias","title":"Librer\u00edas","text":"<p>Para empezar se importan las librer\u00edas requeridas, las cadenas de conexi\u00f3n connections_start y connections_final, y las funciones db_final y lista_tablas de Funciones Generales</p> <pre><code>import pyodbc\nimport pandas as pd\nimport os\nimport time\nfrom funciones_generales import connections_start,connections_final,db_final,lista_tablas\n</code></pre>"},{"location":"01.Repositorio/Scripts_Python/01.Creacion_Bases_Repositorio/#configuracion-para-logs-y-scripts-sql","title":"Configuraci\u00f3n para Logs y scripts SQL","text":"<p>En esta parte se identifica si existe la carpeta para guardar los logs. Si no existe la crea autom\u00e1ticamente. <pre><code># Directorio para guardar logs (opcional)\noutput_dir = os.path.join(os.getcwd(), \"logs\",\"01.Creacion_Repositorio\")\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n    print(\"Directorio creado:\", output_dir)\n</code></pre> Luego configura el entorno para guardar el log del proceso y registra la hora de inicio.</p> <p><pre><code># Limpiar los archivos de log al inicio del proceso\nopen(os.path.join(output_dir, \"01.log.log\"), \"w\", encoding=\"latin1\").close()\n\n# Definir rutas y abrir los archivos de log (modo append)\nlog_path = os.path.join(output_dir, \"01.log.log\")\nlog = open(log_path, \"a\", encoding=\"latin1\")\n\n\n# Tiempo de inicio del proceso\nstart_time = time.time()\nprint(\"Inicio del proceso:\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\nlog.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - Inicio del proceso\\n\")\n</code></pre> Con esta configuraci\u00f3n, valida si la carpeta 03.Scripts_SQL existe o no. Si no existe la crea autom\u00e1ticamente. Esta carpeta es donde se guardan los scripts 01.COL_CTY_PARAMETRICA y 01.CTY_PRI_PARAMETRICA que permiten crear las bases del repositorio</p> <pre><code># Definir la carpeta de destino para los scripts SQL\nCURRENT_DIR = os.getcwd()\nPARENT_DIR = os.path.dirname(CURRENT_DIR)\ndestino = \"03.Scripts_SQL\"\ndestino = os.path.join(PARENT_DIR , destino)\nprint(f\"{destino=}\")\nif not os.path.exists(destino):\n    print(\"No existe la carpeta, se crear\u00e1.\")\n</code></pre>"},{"location":"01.Repositorio/Scripts_Python/01.Creacion_Bases_Repositorio/#scripts-para-tablas-de-metadatos-y-tablas-adicionales-para-el-tablero","title":"Scripts para tablas de metadatos y tablas adicionales para el tablero","text":"<p>En cuanto a los metadatos se crean las tablas metadata_tablas, metadata_campos y metadata_relaciones que contienen los metadatos de las tablas, campos y llaves for\u00e1neas, respectivamente. En el caso de metadata_tablas se incluyen las reglas de negocio definidas para la frecuencia y fecha de actualizaci\u00f3n en el Documento de sugerencias en Gobierno de datos asociado con la calidad de los datos param\u00e9tricos de la historia cl\u00ednica <pre><code># Scripts de creaci\u00f3n de las tablas de metadatos\nmetadata_tablas = \"\"\"\nCREATE TABLE dbo.Metadata_Tablas (\n    Base VARCHAR(255),\n    Esquema VARCHAR(128),\n    Tabla VARCHAR(255),\n    Descripcion_Tabla VARCHAR(255) NULL,\n    create_date DATETIME NULL,\n    modify_date DATETIME NULL,\n    last_user_update DATETIME NULL,\n    DataOwner VARCHAR(255) NULL,\n    ContactEmail VARCHAR(255) NULL,\n    FrecuenciaActualizacion VARCHAR(255) DEFAULT('ANUAL'), \n    FechaActualizacion VARCHAR(255) DEFAULT('01-FEBRERO')\n) ON [PRIMARY];\n\"\"\"\n\n\nmetadata_campos = \"\"\"\nCREATE TABLE dbo.Metadata_Campos (\n    Base VARCHAR(255),     -- Nombre de la base de datos\n    Esquema VARCHAR(128),        -- Nombre del esquema\n    Tabla VARCHAR(255),         -- Nombre de la tabla\n    Campo VARCHAR(128),         -- Nombre del campo\n    Descripcion_Campo VARCHAR(255) NULL,  -- Descripci\u00f3n del campo\n    DataType VARCHAR(128),          -- Tipo de dato\n    Longitud INT NULL,              -- Longitud (para tipos de dato que la requieren)\n    Create_date DATETIME NULL, -- Fecha de creaci\u00f3n\n    Modify_date DATETIME NULL  -- Fecha de modificaci\u00f3n\n) ON [PRIMARY];\n\"\"\"\n\nmetadata_relaciones = \"\"\"\nCREATE TABLE dbo.Metadata_Relaciones (\n    Tabla_Foranea VARCHAR(128),\n    Columna_Foranea VARCHAR(128),\n    Tabla_Primaria VARCHAR(128),\n    Columna_Primaria VARCHAR(128),\n    NombreRestriccion VARCHAR(128) NULL\n) ON [PRIMARY];\n\"\"\"\n</code></pre> Adicionalmente, se crean las siguientes tablas que permiten tomar los datos anteriores y llevarlas a las estructuras que el tablero requiere:</p> <pre><code>dim_componente_calidad = \"\"\"\nCREATE TABLE dbo.Dim_Componente_Calidad (\n    Id_Componente INT,     \n    Componente VARCHAR(255),        \n    Descripcion_Componente VARCHAR(255),        \n    Acciones_Generales VARCHAR(255)\n) ON [PRIMARY];\n\n\"\"\"\nfact_indicadores_historicos = \"\"\"\nCREATE TABLE dbo.Fact_Indicadores_Historicos (\n    Id_Registro INT IDENTITY(1,1),     \n    Id_Componente INT,\n    Base VARCHAR(255),     -- Nombre de la base de datos\n    Esquema VARCHAR(128),        -- Nombre del esquema\n    Tabla VARCHAR(255),         -- Nombre de la tabla\n    Fecha_Medida DATETIME DEFAULT GETDATE(),       \n    Valor_Indicador FLOAT\n) ON [PRIMARY];\n\"\"\"\nfact_metadatos_campos = \"\"\"\nCREATE TABLE dbo.Fact_Metadatos_Campos (\n    Base VARCHAR(255),     -- Nombre de la base de datos\n    Esquema VARCHAR(128),        -- Nombre del esquema\n    Tabla VARCHAR(255),         -- Nombre de la tabla\n    Campo VARCHAR(128),         -- Nombre del campo\n    Descripcion_Campo VARCHAR(255) NULL,  -- Descripci\u00f3n del campo\n    DataType VARCHAR(128),          -- Tipo de dato\n    Longitud INT NULL,              -- Longitud (para tipos de dato que la requieren)\n    Create_date DATETIME NULL, -- Fecha de creaci\u00f3n\n    Modify_date DATETIME NULL,  -- Fecha de modificaci\u00f3n\n    TotalRegistros INT,\n    Nulos INT,\n    Campo_Repetido VARCHAR(255),\n    Metadatos_Completos VARCHAR(255),\n    Registros_Sin_Exactitud INT\n) ON [PRIMARY];\n\"\"\"\n\nfact_metadatos_tablas = \"\"\"\nCREATE TABLE dbo.Fact_Metadatos_Tablas (\n    Base VARCHAR(255),\n    Esquema VARCHAR(128),\n    Tabla VARCHAR(255),\n    Descripcion_Tabla VARCHAR(255) NULL,\n    create_date DATETIME NULL,\n    modify_date DATETIME NULL,\n    last_user_update DATETIME NULL,\n    DataOwner VARCHAR(255) NULL,\n    ContactEmail VARCHAR(255) NULL,\n    FrecuenciaActualizacion VARCHAR(255) DEFAULT('ANUAL'),\n    FechaActualizacion VARCHAR(255) DEFAULT('01-FEBRERO'),\n    TotalRegistros INT,\n    Duplicados INT,\n    Metadatos_Completos VARCHAR(255),\n    Grupo VARCHAR(255),\n    Descripcion_Grupo VARCHAR(255),\n    Ultima_Actualizacion_Requerida DATETIME NULL,\n    Actualizacion_Vigente VARCHAR(255)\n) ON [PRIMARY]\n\"\"\"\n</code></pre>"},{"location":"01.Repositorio/Scripts_Python/01.Creacion_Bases_Repositorio/#funciones-particulares","title":"Funciones particulares","text":"<p>Para este proceso en particular se definen las siguientes funciones:</p> <pre><code>## Guardar el script SQL generado\ndef guardar_script(nombre_archivo, contenido):\n    ruta = os.path.join(destino, nombre_archivo)\n    with open(ruta, \"w\", encoding=\"latin1\") as f:       ## Codificaci\u00f3n latin1\n        f.write(contenido)\n    print(f\"Script {nombre_archivo} generado en {ruta}.\")\n    #log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - Script {nombre_archivo} generado en {ruta}.\\n\")\n</code></pre> <pre><code>## Permite extraer la estructura de la tabla. Esto son su campos y tipo de dato para crear el script SQL\ndef estructura_tabla(cursor, esquema, tabla):\n    # Consulta para obtener la estructura de la tabla\n    try:\n        query = f\"\"\"\n            SELECT COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH, IS_NULLABLE\n            FROM INFORMATION_SCHEMA.COLUMNS\n            WHERE TABLE_NAME = '{tabla}'\n            \"\"\"\n        cursor.execute(query)\n\n        # Generar el script CREATE TABLE\n        create_table_script = f\"CREATE TABLE {esquema}.{tabla} (\\n\"\n        for row in cursor.fetchall():\n            column_name = row.COLUMN_NAME\n            data_type = row.DATA_TYPE\n            max_length = row.CHARACTER_MAXIMUM_LENGTH\n            is_nullable = row.IS_NULLABLE\n\n            # Construir la definici\u00f3n de columna\n            column_definition = f\"  {column_name} {data_type}\"\n\n            # Agregar tama\u00f1o si el tipo de dato lo requiere\n            if max_length == -1:  # Longitud ilimitada\n                column_definition += f\"(MAX)\"\n            elif max_length and data_type in ('varchar', 'char', 'nvarchar', 'binary', 'varbinary'):\n                column_definition += f\"({max_length})\"\n            elif data_type in ('decimal', 'numeric'):  # Decimales requieren precisi\u00f3n y escala\n                column_definition += \"(18, 2)\"  # Longitud predeterminada, ajustable seg\u00fan necesidad\n\n            # Manejar NULL/NOT NULL\n            if is_nullable == \"NO\":\n                column_definition += \" NOT NULL\"\n            else:\n                column_definition += \" NULL\"\n\n            # A\u00f1adir columna al script\n            create_table_script += column_definition + \",\\n\"\n\n        # Eliminar la coma final y cerrar la definici\u00f3n\n        create_table_script = create_table_script.rstrip(\",\\n\") + \"\\n)\"\n        return create_table_script\n    except Exception as e:\n        print(f\"Error generando la estructura para la tabla {tabla}: {e}\")\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - Error generando la estructura para la tabla {tabla}: {e}\\n\")\n        return None\n</code></pre> <pre><code>## Para crear la tabla se valida si existe una tabla y la elimina. Esto permite tener la estructura m\u00e1s reciente\ndef eliminar_tabla(esquema, tabla):\n    # Generar el script SQL para eliminar la tabla si existe\n    try:\n        query = f\"\"\"IF OBJECT_ID('{esquema}.{tabla}', 'U') IS NOT NULL\n        BEGIN\n            DROP TABLE {esquema}.{tabla};\n        END\n        GO\n        \"\"\"\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - Script para eliminar {tabla} incorporado\\n\")\n        return query\n    except Exception as e:\n        print(f\"Error generando el script para eliminar la tabla {tabla}: {e}\")\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - Error generando el script para eliminar la tabla {tabla}: {e}\\n\")\n        return None\n</code></pre>"},{"location":"01.Repositorio/Scripts_Python/01.Creacion_Bases_Repositorio/#proceso-principal","title":"Proceso principal","text":"<p>Se define la conexi\u00f3n a las bases de origen con las cadenas de conexi\u00f3n definidas en connections_start <pre><code># Define conexi\u00f3n para las bases de datos, utilizando cadenas raw para evitar errores de escape\nconnections = connections_start\n</code></pre></p>"},{"location":"01.Repositorio/Scripts_Python/01.Creacion_Bases_Repositorio/#lectura-de-tablas-seleccionadas-para-el-repositorio","title":"Lectura de tablas seleccionadas para el repositorio","text":"<p>Se lee el archivo de Tablas \u00danicas con el listado de tablas a incluir en el repositorio <pre><code># Definir ruta de carpeta con tablas para repositorio\nruta_excel = os.path.join(PARENT_DIR , \"01.Fuentes\",\"01.Tablas_Unicas.xlsx\")\n\n# Leer el archivo Excel\ndf = pd.read_excel(ruta_excel)\n\n# Filtrar las tablas donde En_Repositorio es \"SI\"\ntablas_filtradas = df[df['En_Repositorio'] == 'SI']['Tabla'].tolist()\n</code></pre></p>"},{"location":"01.Repositorio/Scripts_Python/01.Creacion_Bases_Repositorio/#iteracion-por-tabla-y-consolidacion-del-script-sql","title":"Iteraci\u00f3n por tabla y consolidaci\u00f3n del script SQL","text":"<p>Para cada base de datos en la cadena de conexi\u00f3n de las bases de origen se genera el script SQL de las tablas seleccionadas. Adem\u00e1s se incluyen las tablas de metadatos y las adicionales que necesita el tablero. <pre><code># Conectar a las bases y generar scripts\nesquema = 'dbo'\nfor db_name, conn_str in connections.items():\n    print(f\"Procesando base de datos: {db_name}\")\n    log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - Procesando base de datos: {db_name}\\n\")\n    try:\n        conn = pyodbc.connect(conn_str)\n        cursor = conn.cursor()\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - Conexi\u00f3n exitosa a {db_name}\\n\")\n\n        # Generar el script para la base de datos\n        script_db = \"\"\n        db_name_parametrica = db_name + \"_PARAMETRICA\"\n        script_db += f\"USE {db_name_parametrica}\\nGO\\nSET ANSI_NULLS ON\\nGO\\nSET QUOTED_IDENTIFIER ON\\nGO\\n\"\n\n        for tabla in tablas_filtradas:\n            # Eliminar la tabla si existe\n            script_eliminar = eliminar_tabla(esquema, tabla)\n            if script_eliminar:\n                script_db += script_eliminar + \"\\nGO\\n\"\n\n            # Crear la tabla\n            script_tabla = estructura_tabla(cursor, esquema, tabla)\n            if script_tabla:\n                script_db += script_tabla + \" ON [PRIMARY]\\nGO\\n\"\n                log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - Script para crear {tabla} incorporado\\n\")\n\n        ##Incluir setencias de metadatos   \n        script_db += eliminar_tabla(esquema, \"Metadata_Tablas\")\n        script_db += metadata_tablas + \"\\nGO\\n\"\n\n        script_db += eliminar_tabla(esquema, \"Metadata_Campos\")\n        script_db += metadata_campos + \"\\nGO\\n\"\n\n        script_db += eliminar_tabla(esquema, \"Metadata_Relaciones\")        \n        script_db += metadata_relaciones + \"\\nGO\\n\"\n\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - Script para crear metadatos incorporado\\n\")\n\n        ##Incluir setencias de tablas adicionales para el tablero\n\n        script_db += eliminar_tabla(esquema, \"Dim_Componente_Calidad\")        \n        script_db += dim_componente_calidad + \"\\nGO\\n\"\n\n        script_db += eliminar_tabla(esquema, \"Fact_Metadatos_Tablas\")        \n        script_db += fact_metadatos_tablas + \"\\nGO\\n\"\n\n        script_db += eliminar_tabla(esquema, \"Fact_Metadatos_Campos\")        \n        script_db += fact_metadatos_campos + \"\\nGO\\n\"\n\n        script_db += eliminar_tabla(esquema, \"Fact_Indicadores_Historicos\")        \n        script_db += fact_indicadores_historicos + \"\\nGO\\n\"\n\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - Script para crear tablas para tablero incorporado\\n\")             \n\n\n        # Guardar el script en un archivo\n        archivo_script = f\"01.{db_name}_PARAMETRICA.sql\"\n        ruta_archivo = os.path.join(destino, archivo_script)\n        with open(ruta_archivo, \"w\", encoding=\"latin1\") as file:\n            file.write(script_db)\n\n        print(f\"Script generado para {db_name}: 01.{db_name}_PARAMETRICA.sql\")\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - Script generado para {db_name}: 01.{db_name}_PARAMETRICA.sql\\n\")\n        conn.close()\n\n    except Exception as e:\n        print(f\"Error procesando la base de datos {db_name}: {e}\")\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - Error procesando la base de datos {db_name}: {e}\\n\")\n\nprint(\"\u00a1Scripts de creaci\u00f3n generados con \u00e9xito!\")\nlog.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} -\u00a1Scripts de creaci\u00f3n generados con \u00e9xito!\\n\") \n</code></pre></p>"},{"location":"01.Repositorio/Scripts_Python/01.Creacion_Bases_Repositorio/#fin-del-proceso","title":"Fin del proceso","text":"<p>Finalmente, se registra el fin del proceso y se guarda el log. <pre><code>#tiempo total\nend_time = time.time()\ntotal_duration = end_time - start_time\nprint(\"Tiempo total de ejecuci\u00f3n:\", time.strftime(\"%H:%M:%S\", time.gmtime(total_duration)))\nlog.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - Tiempo total de ejecuci\u00f3n: {time.strftime('%H:%M:%S', time.gmtime(total_duration))}\\n\")\n\n#Cerrar log\nlog.close()\n</code></pre></p>"},{"location":"01.Repositorio/Scripts_Python/01.Creacion_Bases_Repositorio/#tiempos-de-procesamiento","title":"Tiempos de procesamiento","text":"<p>Con un equipo con 32 GB de RAM y procesador 11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz 1.69 GHz, el tiempo de ejecuci\u00f3n promedio es de 20 segundos.</p>"},{"location":"01.Repositorio/Scripts_Python/02.Poblar_Bases_Parametricas_Actualizacion/","title":"02 Poblar Bases Param\u00e9tricas Actualizaci\u00f3n","text":"<p>El objetivo de este script es actualizar el repositorio centralizado a partir de las bases COL_CTY y CTY_PRI. La frecuencia sugerida es mensual y est\u00e1 a cargo del equipo de anal\u00edtica de Country.</p>"},{"location":"01.Repositorio/Scripts_Python/02.Poblar_Bases_Parametricas_Actualizacion/#librerias","title":"Librer\u00edas","text":"<p>Para empezar se importan las librer\u00edas requeridas, las cadenas de conexi\u00f3n connections_start y connections_final, y las funciones db_final y lista_tablas de Funciones Generales <pre><code>import pyodbc\nimport pandas as pd\nimport os\nimport time\nfrom funciones_generales import connections_start,connections_final,db_final,lista_tablas\n</code></pre></p>"},{"location":"01.Repositorio/Scripts_Python/02.Poblar_Bases_Parametricas_Actualizacion/#configuracion-para-logs","title":"Configuraci\u00f3n para Logs","text":"<p>En esta parte se identifica si existe la carpeta para guardar los logs. Si no existe la crea autom\u00e1ticamente. Adem\u00e1s, se da inicio al proceso. <pre><code># Directorio para guardar logs (opcional)\noutput_dir = os.path.join(os.getcwd(), \"logs\",\"02.Poblar_Bases_Actualizacion\")\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n    print(\"Directorio creado:\", output_dir)\n\n\n\n# Limpiar los archivos de log al inicio del proceso\nopen(os.path.join(output_dir, \"01.log.log\"), \"w\", encoding=\"latin1\").close()\n\n# Definir rutas y abrir los archivos de log (modo append)\nlog_path = os.path.join(output_dir, \"01.log.log\")\nlog = open(log_path, \"a\", encoding=\"latin1\")\n\n\n\n# Tiempo de inicio del proceso\nstart_time = time.time()\nprint(\"Inicio del proceso de poblamiento de resultados:\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\nprint(\"Directorio de trabajo:\", os.getcwd())\nlog.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Inicio del proceso de poblamiento de resultados\\n\")\n</code></pre></p>"},{"location":"01.Repositorio/Scripts_Python/02.Poblar_Bases_Parametricas_Actualizacion/#poblar-tablas","title":"Poblar tablas","text":"<p>Para actualizar las tablas, se consultan las tablas definidas para el repositorio en las bases de origen COL_CTY y CTY_PRI y se llevan a las bases de destino COL_CTY_PARAMETRICA y CTY_PRI_PARAMETRICA.</p> <pre><code># Conectar a las bases y poblar tablas\nesquema = 'dbo'\nfor db_name, conn_str in connections_start.items():\n    print(f\"Procesando base de datos: {db_name}\")\n    log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Procesando base de datos: {db_name}\\n\")\n    #print(conn_str)\n    try:\n        # Conexion a base fuente\n        conn = pyodbc.connect(conn_str)\n        cursor = conn.cursor()\n\n        #Conexion a base destino\n\n        db_name_final = db_final(db_name)\n        conn_str_final = connections_final[db_name_final]\n        conn_final = pyodbc.connect(conn_str_final)\n        cursor_final = conn_final.cursor() \n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Conexi\u00f3n a {db_name_final} exitosa \\n\")\n\n        tablas_filtradas = lista_tablas(db_name_final,cursor_final)\n        for nombre_tabla in tablas_filtradas:\n            query = f\"SELECT * FROM dbo.{nombre_tabla};\"\n            cursor.execute(query)\n            #print(nombre_tabla)\n\n            # Obtener los resultados de la consulta\n            rows = cursor.fetchall()\n            columns = [column[0] for column in cursor.description]  # Obtener los nombres de las columnas\n\n\n            try:\n                clean_query = f\"DELETE FROM dbo.{nombre_tabla};\"\n                cursor_final.execute(clean_query)\n                conn_final.commit()\n                print(f\"Tabla dbo.{nombre_tabla} limpiada exitosamente\")\n                log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Tabla dbo.{nombre_tabla} limpiada exitosamente\\n\")\n                #sql_script += f\"-- Limpiar tabla {nombre_tabla_sql}\\n\" + clean_query + \"\\n\"\n            except Exception as e:\n                print(f\"Error al limpiar la tabla dbo.{nombre_tabla}: {e}\")\n                log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: Al limpiar la tabla dbo.{nombre_tabla}: {e}\\n\")\n\n            try:\n                # Construir una consulta de inserci\u00f3n din\u00e1mica para cada fila\n                for row in rows:\n                    values_placeholder = \", \".join([\"?\"] * len(columns))  # Generar placeholders para los valores\n                    insert_query = f\"INSERT INTO {esquema}.{nombre_tabla} ({', '.join(columns)}) VALUES ({values_placeholder})\"\n                    cursor_final.execute(insert_query, row)\n                    conn_final.commit()\n\n                # Confirmar los cambios en la base de datos destino\n                conn_final.commit()\n                print(f\"Datos de la tabla {nombre_tabla} transferidos a {db_name_final}.\")\n                log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Datos de la tabla {nombre_tabla} transferidos a {db_name_final}.\\n\")\n            except Exception as e:\n                print(f\"Error enviando la tabla {nombre_tabla} a {db_name_final}: {e}\")\n                log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: Enviando la tabla {nombre_tabla} a {db_name_final}: {e}\\n\")\n\n    except Exception as e:\n        print(f\"Error procesando la base de datos {db_name}: {e}\")\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: Procesando la base de datos {db_name}: {e}\\n\")\n</code></pre>"},{"location":"01.Repositorio/Scripts_Python/02.Poblar_Bases_Parametricas_Actualizacion/#funciones-para-metadatos","title":"Funciones para metadatos","text":"<p>Para la extracci\u00f3n de los metadatos se definen las siguientes funciones que permiten hacer las consultas directamente en el motor SQL:</p> <pre><code># Consulta para los metadatos de las tablas\ndef consulta_metadatos_tablas(db_name):\n    query_metadatos = f\"\"\"WITH Tablas as (    \nSELECT\n    TABLE_CATALOG AS Base,\n    TABLE_SCHEMA AS Esquema,\n    TABLE_NAME AS Tabla\nFROM \n    {db_name}.INFORMATION_SCHEMA.TABLES\nWHERE TABLE_SCHEMA = 'dbo' ),\nTablasConDescripcion AS (\n    SELECT\n        obj.name AS Tabla,\n        ep.name AS Propiedad,\n        --ep.value AS Descripcion_Tabla,\n        CAST(ep.value AS NVARCHAR(MAX)) AS Descripcion_Tabla,\n        '{db_name}' as Base\n    FROM\n        {db_name}.sys.extended_properties AS ep\n        INNER JOIN {db_name}.sys.objects AS obj ON ep.major_id = obj.object_id\n    WHERE\n        obj.schema_id = SCHEMA_ID('dbo')  -- Cambia 'dbo' por el esquema que necesites\n        AND ep.minor_id = 0  -- Indica la tabla\n        AND ep.name = 'MS_Description'\n),\nActualizacionRegistros AS (\nSELECT \n    DB_NAME(s.database_id) AS Base,\n    t.name AS Tabla,\n    s.last_user_update \nFROM \n    {db_name}.sys.tables t\nLEFT JOIN \n    {db_name}.sys.dm_db_index_usage_stats s\nON \n    t.object_id = s.object_id\n),\nDataOwner AS (\nSELECT\n        obj.name AS Tabla,\n        ep.name AS Propiedad,\n        --ep.value AS DataOwner,\n        CAST(ep.value AS NVARCHAR(MAX)) AS DataOwner,\n        '{db_name}' as Base\n    FROM\n        {db_name}.sys.extended_properties AS ep\n        INNER JOIN {db_name}.sys.objects AS obj ON ep.major_id = obj.object_id\n    WHERE\n        obj.schema_id = SCHEMA_ID('dbo')  -- Cambia 'dbo' por el esquema que necesites\n        AND ep.minor_id = 0  -- Indica la tabla\n        AND ep.name = 'DataOwner'\n),\nContactEmail AS (\n    SELECT\n        obj.name AS Tabla,\n        ep.name AS Propiedad,\n        --ep.value AS ContactEmail,\n        CAST(ep.value AS NVARCHAR(MAX)) AS ContactEmail,\n        '{db_name}' as Base\n    FROM\n        {db_name}.sys.extended_properties AS ep\n        INNER JOIN {db_name}.sys.objects AS obj ON ep.major_id = obj.object_id\n    WHERE\n        obj.schema_id = SCHEMA_ID('dbo')  -- Cambia 'dbo' por el esquema que necesites\n        AND ep.minor_id = 0  -- Indica la tabla\n        AND ep.name = 'ContactEmail'\n),\nFechasMetadatos AS (\n    SELECT \n        '{db_name}' as Base,\n        name AS Tabla,\n        create_date,\n        modify_date\n    FROM \n        {db_name}.sys.objects\n    WHERE \n        type = 'U'  -- Esto filtra solo las tablas de usuario\n        AND schema_id = SCHEMA_ID('dbo')\n)\nSELECT \n    t.Base,\n    t.Esquema,\n    t.Tabla,\n    td.Descripcion_Tabla,\n    fm.create_date,\n    fm.modify_date,\n    ar.last_user_update,\n    do.DataOwner,\n    ce.ContactEmail\nFROM Tablas t\nLEFT JOIN TablasConDescripcion td ON t.Base = td.Base and t.Tabla = td.Tabla\nLEFT JOIN ActualizacionRegistros ar ON t.Base = ar.Base and t.Tabla = ar.Tabla\nLEFT JOIN DataOwner do ON t.Base = do.Base and t.Tabla = do.Tabla\nLEFT JOIN ContactEmail ce ON t.Base = ce.Base and t.Tabla = ce.Tabla\nLEFT JOIN FechasMetadatos fm ON t.Base = fm.Base and t.Tabla = fm.Tabla\nINNER JOIN {db_name}_PARAMETRICA.sys.tables t_p ON t.Tabla = t_p.name\nORDER BY t.Tabla\n\"\"\"\n    return query_metadatos\n</code></pre> <pre><code>#Consulta para los metadatos de los campos\ndef consulta_metadatos_campos(db_name):\n    query_metadatos = f\"\"\"WITH Columnas AS (\nSELECT \n    TABLE_CATALOG AS Base,\n    TABLE_SCHEMA AS Esquema,\n    TABLE_NAME AS Tabla,\n    COLUMN_NAME AS Columna,\n    DATA_TYPE,\n    CHARACTER_MAXIMUM_LENGTH\nFROM \n    {db_name}.INFORMATION_SCHEMA.COLUMNS\nWHERE TABLE_SCHEMA = 'dbo'\n),\nColumnasConDescripcion AS (\n    SELECT\n        ep.name AS Propiedad,\n        --ep.value AS Descripcion_Campo,\n        CAST(ep.value AS NVARCHAR(MAX))AS Descripcion_Campo,\n        obj.name AS Tabla,\n        col.name AS Columna,\n        '{db_name}' as Base\n        --typ.name AS TipoDato,\n    FROM\n        {db_name}.sys.extended_properties AS ep\n        INNER JOIN {db_name}.sys.objects AS obj ON ep.major_id = obj.object_id\n        INNER JOIN {db_name}.sys.columns AS col ON ep.major_id = col.object_id AND ep.minor_id = col.column_id\n        INNER JOIN {db_name}.sys.types AS typ ON col.user_type_id = typ.user_type_id\n),\nFechasMetadatos AS (\n    SELECT \n        '{db_name}' as Base,\n        obj.name AS Tabla,\n        col.name AS Columna,\n        obj.create_date,\n        obj.modify_date\n    FROM \n        {db_name}.sys.objects obj\n    INNER JOIN \n        {db_name}.sys.columns col ON obj.object_id = col.object_id\n    WHERE \n        obj.schema_id = SCHEMA_ID('dbo')  -- Ajusta el esquema si es necesario\n)\nSELECT \n    c.Base,\n    c.Esquema,\n    c.Tabla,\n    c.Columna as Campo,\n    cd.Descripcion_Campo,\n    c.DATA_TYPE as DataType,\n    c.CHARACTER_MAXIMUM_LENGTH as Longitud,\n    fm.create_date,\n    fm.modify_date\nFROM Columnas c\nLEFT JOIN ColumnasConDescripcion cd ON c.Base = cd.Base and c.Tabla = cd.Tabla and c.Columna = cd.Columna\nLEFT JOIN FechasMetadatos fm ON c.Base = fm.Base and c.Tabla = fm.Tabla and c.Columna = fm.Columna\nINNER JOIN {db_name}_PARAMETRICA.sys.tables t_p ON c.Tabla = t_p.name\norder by c.tabla\n\"\"\"\n    return query_metadatos\n</code></pre> <p><pre><code># Consulta para los metadatos de las llaves for\u00e1neas \ndef consulta_metadatos_relaciones(db_name):\n    query_metadatos = f\"\"\"SELECT         \n               pk_tab.name AS Tabla_Primaria,\n               pk_col.name AS Columna_Primaria,\n               fk_tab.name AS Tabla_Foranea, \n               fk_col.name AS Columna_Foranea, \n               fk.name AS NombreRestriccion\n        FROM {db_name}.sys.foreign_keys fk\n        INNER JOIN {db_name}.sys.foreign_key_columns fkc ON fk.object_id = fkc.constraint_object_id\n        INNER JOIN {db_name}.sys.tables fk_tab ON fk_tab.object_id = fkc.parent_object_id\n        INNER JOIN {db_name}.sys.columns fk_col ON fk_col.column_id = fkc.parent_column_id AND fk_col.object_id = fk_tab.object_id\n        INNER JOIN {db_name}.sys.tables pk_tab ON pk_tab.object_id = fkc.referenced_object_id\n        INNER JOIN {db_name}.sys.columns pk_col ON pk_col.column_id = fkc.referenced_column_id AND pk_col.object_id = pk_tab.object_id\n        INNER JOIN {db_name}_PARAMETRICA.sys.tables t_p ON pk_tab.name = t_p.name\n\"\"\"\n    return query_metadatos\n</code></pre> Adem\u00e1s, se definen los siguientes diccionarios para calcular los metadatos de acuerdo a la conexi\u00f3n realizada</p> <pre><code>## Tablas_Destino\n\ntablas_fuente ={\n    \"tablas_met_COL\" : consulta_metadatos_tablas(\"COL_CTY\"),\n    \"campos_met_COL\" : consulta_metadatos_campos(\"COL_CTY\"),\n    \"relaciones_met_COL\" : consulta_metadatos_relaciones(\"COL_CTY\"),\n    \"tablas_met_CTY\" : consulta_metadatos_tablas(\"CTY_PRI\"),\n    \"campos_met_CTY\" : consulta_metadatos_campos(\"CTY_PRI\"),\n    \"relaciones_met_CTY\" : consulta_metadatos_relaciones(\"CTY_PRI\")\n}\n\ntablas_base ={\n    \"tablas_met_COL\" :\"COL_CTY\",\n    \"campos_met_COL\": \"COL_CTY\",\n    \"relaciones_met_COL\": \"COL_CTY\",\n    \"tablas_met_CTY\" :\"CTY_PRI\",\n    \"campos_met_CTY\": \"CTY_PRI\",\n    \"relaciones_met_CTY\": \"CTY_PRI\"\n}\n\ntablas_final ={\n    \"tablas_met_COL\" :\"dbo.Metadata_Tablas\",\n    \"campos_met_COL\": \"dbo.Metadata_Campos\",\n    \"relaciones_met_COL\": \"dbo.Metadata_Relaciones\",\n    \"tablas_met_CTY\" :\"dbo.Metadata_Tablas\",\n    \"campos_met_CTY\": \"dbo.Metadata_Campos\",\n    \"relaciones_met_CTY\": \"dbo.Metadata_Relaciones\"\n}\n</code></pre>"},{"location":"01.Repositorio/Scripts_Python/02.Poblar_Bases_Parametricas_Actualizacion/#procesamiento-de-metadatos","title":"Procesamiento de metadatos","text":"<p>Para la informaci\u00f3n de los metadatos se hacen las consultas a partir de las funciones anteriores en las bases de origen COL_CTY y CTY_PRI y los resultados se llevan a las bases de destino COL_CTY_PARAMETRICA y CTY_PRI_PARAMETRICA.</p> <pre><code>for tabla in tablas_fuente.keys():\n    #Conexion a base destino\n    db_name = tablas_base[tabla]  \n    db_name_final = db_final(db_name)\n    conn_str_final = connections_final[db_name_final]\n    conn_final = pyodbc.connect(conn_str_final)\n    cursor_final = conn_final.cursor()\n    log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Conexi\u00f3n a base {db_name_final} realizada con \u00e9xito\\n\")\n\n    print(tabla)\n    try:\n        clean_query = f\"DELETE FROM {tablas_final[tabla]};\"\n        cursor_final.execute(clean_query)\n        conn_final.commit()\n        print(f\"Tabla {tablas_final[tabla]} limpiada exitosamente\")\n\n    except Exception as e:\n        print(f\"Error al limpiar la tabla {tablas_final[tabla]}: {e}\")\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: Al limpiar la tabla {tablas_final[tabla]}: {e}\\n\")\n\n    # Obtener los resultados de la consulta\n    query = tablas_fuente[tabla]\n    cursor_final.execute(query)\n    rows = cursor_final.fetchall()\n    columns = [column[0] for column in cursor_final.description]  # Obtener los nombres de las columnas    \n\n\n    try:\n        # Construir una consulta de inserci\u00f3n din\u00e1mica para cada fila      \n        for row in rows:\n            values_placeholder = \", \".join([\"?\"] * len(columns))  # Generar placeholders para los valores\n            insert_query = f\"INSERT INTO {tablas_final[tabla]} ({', '.join(columns)}) VALUES ({values_placeholder})\"\n            cursor_final.execute(insert_query, row)\n            conn_final.commit()\n\n            # Confirmar los cambios en la base de datos destino\n            conn_final.commit()\n        print(f\"Datos de la tabla {tablas_final[tabla]} transferidos a {db_name_final}.\")\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Datos de la tabla {tablas_final[tabla]} transferidos a {db_name_final}.\\n\")\n    except Exception as e:\n        print(f\"Error enviando la tabla {tablas_final[tabla]} a {db_name_final}: {e}\")\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: Enviando la tabla {tablas_final[tabla]} a {db_name_final}: {e}\\n\")\n</code></pre>"},{"location":"01.Repositorio/Scripts_Python/02.Poblar_Bases_Parametricas_Actualizacion/#fin-del-proceso","title":"Fin del proceso","text":"<p>Finalmente, se registra el fin del proceso y se guarda el log.</p> <pre><code>#tiempo total\nend_time = time.time()\ntotal_duration = end_time - start_time\nprint(\"Tiempo total de ejecuci\u00f3n:\", time.strftime(\"%H:%M:%S\", time.gmtime(total_duration)))\nlog.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Tiempo total de ejecuci\u00f3n: {time.strftime('%H:%M:%S', time.gmtime(total_duration))}\\n\")\n\n#Cerrar log\nlog.close()\n</code></pre>"},{"location":"01.Repositorio/Scripts_Python/02.Poblar_Bases_Parametricas_Actualizacion/#tiempos-de-procesamiento","title":"Tiempos de procesamiento","text":"<p>Con un equipo con 32 GB de RAM y procesador 11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz 1.69 GHz, el tiempo de ejecuci\u00f3n promedio es de 18 minutos.</p>"},{"location":"01.Repositorio/Scripts_Python/funciones_generales/","title":"Funciones Generales","text":""},{"location":"01.Repositorio/Scripts_Python/funciones_generales/#funciones-generales","title":"Funciones Generales","text":"<p>EL objetivo de este script es consolidar las funciones que son transversales para las ETLs del repositorio y el tablero. En donde se destacan las cadenas de conexi\u00f3n a las bases de datos y el listado de tablas a incluir.</p>"},{"location":"01.Repositorio/Scripts_Python/funciones_generales/#librerias","title":"Librer\u00edas","text":"<p>Para empezar, se importan las librer\u00edas estandar de python y que vienen incluidas por defecto</p> <p><pre><code>#Librerias Estandar\nimport os\nimport time\nfrom datetime import datetime, date, time as datetime_time\nimport decimal\n</code></pre> Luego se importan las librer\u00edas externas que se hacen parte del entorno virtual explicado anteriormente.</p> <pre><code>#Librerias Externas\nimport pyodbc\nimport pandas as pd\nimport numpy as np\n</code></pre>"},{"location":"01.Repositorio/Scripts_Python/funciones_generales/#cadenas-de-conexion","title":"Cadenas de conexi\u00f3n","text":"<p>De esta manera se definen las cadenas de conexi\u00f3n de origen y destino. Es importante tener en cuenta, que al momento de hacer la instalaci\u00f3n se deben configurar las cadenas con las rutas adecuadas.</p> <pre><code># Diccionario de conexiones\nconnections_start = {\n    \"CTY_PRI\": r\"DRIVER={SQL Server};SERVER=QCS-CONS04;DATABASE=CTY_PRI;Trusted_Connection=yes;\",\n    \"COL_CTY\": r\"DRIVER={SQL Server};SERVER=QCS-CONS04;DATABASE=COL_CTY;Trusted_Connection=yes;\",\n}\n\nconnections_final = {\n    \"CTY_PRI_PARAMETRICA\": r\"DRIVER={SQL Server};SERVER=QCS-CONS04;DATABASE=CTY_PRI_PARAMETRICA;Trusted_Connection=yes;\",\n    \"COL_CTY_PARAMETRICA\": r\"DRIVER={SQL Server};SERVER=QCS-CONS04;DATABASE=COL_CTY_PARAMETRICA;Trusted_Connection=yes;\",\n}\n</code></pre>"},{"location":"01.Repositorio/Scripts_Python/funciones_generales/#funciones-transversales","title":"Funciones Transversales","text":"<p>As\u00ed, se define la siguiente funci\u00f3n que permite identificar la conexi\u00f3n de la base de origen con la base de destino, es decir, la base original y la base param\u00e9trica: <pre><code>def db_final(db_name):\n    if db_name == \"COL_CTY\":\n        db_name_final ='COL_CTY_PARAMETRICA'\n    elif db_name == \"CTY_PRI\":\n        db_name_final ='CTY_PRI_PARAMETRICA'\n    return db_name_final   \n</code></pre> Finalmente, se define la siguiente funci\u00f3n que permite obtener el listado de tablas de la base param\u00e9trica sin tener en cuenta las tablas asociadas a los metadatos o indicadores de calidad: <pre><code>def lista_tablas(db_name_final,cursor):\n    query_tables_list = f\"\"\"SELECT \n            TABLE_NAME\n            FROM        \n        {db_name_final}.INFORMATION_SCHEMA.TABLES\n        WHERE \n        TABLE_TYPE = 'BASE TABLE' \n        AND TABLE_CATALOG = '{db_name_final}'\n        AND TABLE_NAME NOT IN ('Metadata_Tablas','Metadata_Campos','Metadata_Relaciones','Fact_Metadatos_Tablas','Fact_Metadatos_Campos','Fact_Indicadores_Historicos','Dim_Componente_Calidad');\"\"\"\n    # Ejecutar la consulta\n    cursor.execute(query_tables_list)\n\n    # Obtener los resultados de la consulta\n    rows = cursor.fetchall()\n\n    # Extraer los nombres de las tablas en una lista\n    tables_list = [row[0] for row in rows]\n\n    return tables_list\n</code></pre></p>"},{"location":"01.Repositorio/Scripts_SQL/01.COL_CTY_PARAMETRICA/","title":"01.COL_CTY_PARAMETRICA","text":""},{"location":"01.Repositorio/Scripts_SQL/01.COL_CTY_PARAMETRICA/#01col_cty_parametrica","title":"01.COL_CTY_PARAMETRICA","text":"<p>Este script SQL contiene una de las salidas de la ETL 01.Creacion_Bases_Repositorio.md, la cual permite crear la base 01.COL_CTY_PARAMETRICA del repositorio centralizado. El script empieza haciendo uso de la base, configura valores nulos y el uso de las dobles comillas:</p> <p><pre><code>USE COL_CTY_PARAMETRICA\nGO\nSET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\n</code></pre> Luego incluye el script de creaci\u00f3n de cada tabla, elimin\u00e1ndola en caso de que ya se encuentre presente en la base. <pre><code>IF OBJECT_ID('dbo.ACF_ELIN_COMP', 'U') IS NOT NULL\n        BEGIN\n            DROP TABLE dbo.ACF_ELIN_COMP;\n        END\n        GO\n\nGO\nCREATE TABLE dbo.ACF_ELIN_COMP (\n  cod_enti char(2) NOT NULL,\n  tip_elin char(1) NOT NULL,\n  cod_elin int NOT NULL,\n  cod_comp int NOT NULL,\n  can_comp float NOT NULL\n) ON [PRIMARY]\nGO\n</code></pre></p> <p>Despu\u00e9s de abordar las tablas de datos, se hace lo mismo con las metadatos y las tablas adicionales para el tablero. Se incluye su script de creaci\u00f3n, eliminandola en caso de que ya se encuentre presente en la base. <pre><code>IF OBJECT_ID('dbo.Metadata_Tablas', 'U') IS NOT NULL\n        BEGIN\n            DROP TABLE dbo.Metadata_Tablas;\n        END\n        GO\n\nCREATE TABLE dbo.Metadata_Tablas (\n    Base VARCHAR(255),\n    Esquema VARCHAR(128),\n    Tabla VARCHAR(255),\n    Descripcion_Tabla VARCHAR(255) NULL,\n    create_date DATETIME NULL,\n    modify_date DATETIME NULL,\n    last_user_update DATETIME NULL,\n    DataOwner VARCHAR(255) NULL,\n    ContactEmail VARCHAR(255) NULL,\n    FrecuenciaActualizacion VARCHAR(255) DEFAULT('ANUAL'),\n    FechaActualizacion VARCHAR(255) DEFAULT('01-FEBRERO')\n) ON [PRIMARY];\n\nGO\nIF OBJECT_ID('dbo.Metadata_Campos', 'U') IS NOT NULL\n        BEGIN\n            DROP TABLE dbo.Metadata_Campos;\n        END\n        GO\n\nCREATE TABLE dbo.Metadata_Campos (\n    Base VARCHAR(255),     -- Nombre de la base de datos\n    Esquema VARCHAR(128),        -- Nombre del esquema\n    Tabla VARCHAR(255),         -- Nombre de la tabla\n    Campo VARCHAR(128),         -- Nombre del campo\n    Descripcion_Campo VARCHAR(255) NULL,  -- Descripci\u00f3n del campo\n    DataType VARCHAR(128),          -- Tipo de dato\n    Longitud INT NULL,              -- Longitud (para tipos de dato que la requieren)\n    Create_date DATETIME NULL, -- Fecha de creaci\u00f3n\n    Modify_date DATETIME NULL  -- Fecha de modificaci\u00f3n\n) ON [PRIMARY];\n\nGO\nIF OBJECT_ID('dbo.Metadata_Relaciones', 'U') IS NOT NULL\n        BEGIN\n            DROP TABLE dbo.Metadata_Relaciones;\n        END\n        GO\n\nCREATE TABLE dbo.Metadata_Relaciones (\n    Tabla_Foranea VARCHAR(128),\n    Columna_Foranea VARCHAR(128),\n    Tabla_Primaria VARCHAR(128),\n    Columna_Primaria VARCHAR(128),\n    NombreRestriccion VARCHAR(128) NULL\n) ON [PRIMARY];\n\nGO\nIF OBJECT_ID('dbo.Dim_Componente_Calidad', 'U') IS NOT NULL\n        BEGIN\n            DROP TABLE dbo.Dim_Componente_Calidad;\n        END\n        GO\n\nCREATE TABLE dbo.Dim_Componente_Calidad (\n    Id_Componente INT,     \n    Componente VARCHAR(255),        \n    Descripcion_Componente VARCHAR(255),        \n    Acciones_Generales VARCHAR(255)\n) ON [PRIMARY];\n\n\nGO\nIF OBJECT_ID('dbo.Fact_Metadatos_Tablas', 'U') IS NOT NULL\n        BEGIN\n            DROP TABLE dbo.Fact_Metadatos_Tablas;\n        END\n        GO\n\nCREATE TABLE dbo.Fact_Metadatos_Tablas (\n    Base VARCHAR(255),\n    Esquema VARCHAR(128),\n    Tabla VARCHAR(255),\n    Descripcion_Tabla VARCHAR(255) NULL,\n    create_date DATETIME NULL,\n    modify_date DATETIME NULL,\n    last_user_update DATETIME NULL,\n    DataOwner VARCHAR(255) NULL,\n    ContactEmail VARCHAR(255) NULL,\n    FrecuenciaActualizacion VARCHAR(255) DEFAULT('ANUAL'),\n    FechaActualizacion VARCHAR(255) DEFAULT('01-FEBRERO'),\n    TotalRegistros INT,\n    Duplicados INT,\n    Metadatos_Completos VARCHAR(255),\n    Grupo VARCHAR(255),\n    Descripcion_Grupo VARCHAR(255),\n    Ultima_Actualizacion_Requerida DATETIME NULL,\n    Actualizacion_Vigente VARCHAR(255)\n) ON [PRIMARY]\n\nGO\nIF OBJECT_ID('dbo.Fact_Metadatos_Campos', 'U') IS NOT NULL\n        BEGIN\n            DROP TABLE dbo.Fact_Metadatos_Campos;\n        END\n        GO\n\nCREATE TABLE dbo.Fact_Metadatos_Campos (\n    Base VARCHAR(255),     -- Nombre de la base de datos\n    Esquema VARCHAR(128),        -- Nombre del esquema\n    Tabla VARCHAR(255),         -- Nombre de la tabla\n    Campo VARCHAR(128),         -- Nombre del campo\n    Descripcion_Campo VARCHAR(255) NULL,  -- Descripci\u00f3n del campo\n    DataType VARCHAR(128),          -- Tipo de dato\n    Longitud INT NULL,              -- Longitud (para tipos de dato que la requieren)\n    Create_date DATETIME NULL, -- Fecha de creaci\u00f3n\n    Modify_date DATETIME NULL,  -- Fecha de modificaci\u00f3n\n    TotalRegistros INT,\n    Nulos INT,\n    Campo_Repetido VARCHAR(255),\n    Metadatos_Completos VARCHAR(255),\n    Registros_Sin_Exactitud INT\n) ON [PRIMARY];\n\nGO\nIF OBJECT_ID('dbo.Fact_Indicadores_Historicos', 'U') IS NOT NULL\n        BEGIN\n            DROP TABLE dbo.Fact_Indicadores_Historicos;\n        END\n        GO\n\nCREATE TABLE dbo.Fact_Indicadores_Historicos (\n    Id_Registro INT IDENTITY(1,1),     \n    Id_Componente INT,\n    Base VARCHAR(255),     -- Nombre de la base de datos\n    Esquema VARCHAR(128),        -- Nombre del esquema\n    Tabla VARCHAR(255),         -- Nombre de la tabla\n    Fecha_Medida DATETIME DEFAULT GETDATE(),       \n    Valor_Indicador FLOAT\n) ON [PRIMARY];\n\nGO\n</code></pre></p>"},{"location":"01.Repositorio/Scripts_SQL/01.CTY_PRI_PARAMETRICA/","title":"01.CTY_PRI_PARAMETRICA","text":""},{"location":"01.Repositorio/Scripts_SQL/01.CTY_PRI_PARAMETRICA/#01cty_pri_parametrica","title":"01.CTY_PRI_PARAMETRICA","text":"<p>Este script SQL contiene una de las salidas de la ETL 01.Creacion_Bases_Repositorio.md, la cual permite crear la base 01.CTY_PRI_PARAMETRICA del repositorio centralizado. El script empieza haciendo uso de la base, configura valores nulos y el uso de las dobles comillas:</p> <p><pre><code>USE CTY_PRI_PARAMETRICA\nGO\nSET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\n</code></pre> Luego incluye el script de creaci\u00f3n de cada tabla, elimin\u00e1ndola en caso de que ya se encuentre presente en la base. <pre><code>IF OBJECT_ID('dbo.ACF_ELIN_COMP', 'U') IS NOT NULL\n        BEGIN\n            DROP TABLE dbo.ACF_ELIN_COMP;\n        END\n        GO\n\nGO\nCREATE TABLE dbo.ACF_ELIN_COMP (\n  cod_enti char(2) NOT NULL,\n  tip_elin char(1) NOT NULL,\n  cod_elin int NOT NULL,\n  cod_comp int NOT NULL,\n  can_comp float NOT NULL\n) ON [PRIMARY]\nGO\n</code></pre></p> <p>Despu\u00e9s de abordar las tablas de datos, se hace lo mismo con las metadatos y las tablas adicionales para el tablero. Se incluye su script de creaci\u00f3n, eliminandola en caso de que ya se encuentre presente en la base. <pre><code>IF OBJECT_ID('dbo.Metadata_Tablas', 'U') IS NOT NULL\n        BEGIN\n            DROP TABLE dbo.Metadata_Tablas;\n        END\n        GO\n\nCREATE TABLE dbo.Metadata_Tablas (\n    Base VARCHAR(255),\n    Esquema VARCHAR(128),\n    Tabla VARCHAR(255),\n    Descripcion_Tabla VARCHAR(255) NULL,\n    create_date DATETIME NULL,\n    modify_date DATETIME NULL,\n    last_user_update DATETIME NULL,\n    DataOwner VARCHAR(255) NULL,\n    ContactEmail VARCHAR(255) NULL,\n    FrecuenciaActualizacion VARCHAR(255) DEFAULT('ANUAL'),\n    FechaActualizacion VARCHAR(255) DEFAULT('01-FEBRERO')\n) ON [PRIMARY];\n\nGO\nIF OBJECT_ID('dbo.Metadata_Campos', 'U') IS NOT NULL\n        BEGIN\n            DROP TABLE dbo.Metadata_Campos;\n        END\n        GO\n\nCREATE TABLE dbo.Metadata_Campos (\n    Base VARCHAR(255),     -- Nombre de la base de datos\n    Esquema VARCHAR(128),        -- Nombre del esquema\n    Tabla VARCHAR(255),         -- Nombre de la tabla\n    Campo VARCHAR(128),         -- Nombre del campo\n    Descripcion_Campo VARCHAR(255) NULL,  -- Descripci\u00f3n del campo\n    DataType VARCHAR(128),          -- Tipo de dato\n    Longitud INT NULL,              -- Longitud (para tipos de dato que la requieren)\n    Create_date DATETIME NULL, -- Fecha de creaci\u00f3n\n    Modify_date DATETIME NULL  -- Fecha de modificaci\u00f3n\n) ON [PRIMARY];\n\nGO\nIF OBJECT_ID('dbo.Metadata_Relaciones', 'U') IS NOT NULL\n        BEGIN\n            DROP TABLE dbo.Metadata_Relaciones;\n        END\n        GO\n\nCREATE TABLE dbo.Metadata_Relaciones (\n    Tabla_Foranea VARCHAR(128),\n    Columna_Foranea VARCHAR(128),\n    Tabla_Primaria VARCHAR(128),\n    Columna_Primaria VARCHAR(128),\n    NombreRestriccion VARCHAR(128) NULL\n) ON [PRIMARY];\n\nGO\nIF OBJECT_ID('dbo.Dim_Componente_Calidad', 'U') IS NOT NULL\n        BEGIN\n            DROP TABLE dbo.Dim_Componente_Calidad;\n        END\n        GO\n\nCREATE TABLE dbo.Dim_Componente_Calidad (\n    Id_Componente INT,     \n    Componente VARCHAR(255),        \n    Descripcion_Componente VARCHAR(255),        \n    Acciones_Generales VARCHAR(255)\n) ON [PRIMARY];\n\n\nGO\nIF OBJECT_ID('dbo.Fact_Metadatos_Tablas', 'U') IS NOT NULL\n        BEGIN\n            DROP TABLE dbo.Fact_Metadatos_Tablas;\n        END\n        GO\n\nCREATE TABLE dbo.Fact_Metadatos_Tablas (\n    Base VARCHAR(255),\n    Esquema VARCHAR(128),\n    Tabla VARCHAR(255),\n    Descripcion_Tabla VARCHAR(255) NULL,\n    create_date DATETIME NULL,\n    modify_date DATETIME NULL,\n    last_user_update DATETIME NULL,\n    DataOwner VARCHAR(255) NULL,\n    ContactEmail VARCHAR(255) NULL,\n    FrecuenciaActualizacion VARCHAR(255) DEFAULT('ANUAL'),\n    FechaActualizacion VARCHAR(255) DEFAULT('01-FEBRERO'),\n    TotalRegistros INT,\n    Duplicados INT,\n    Metadatos_Completos VARCHAR(255),\n    Grupo VARCHAR(255),\n    Descripcion_Grupo VARCHAR(255),\n    Ultima_Actualizacion_Requerida DATETIME NULL,\n    Actualizacion_Vigente VARCHAR(255)\n) ON [PRIMARY]\n\nGO\nIF OBJECT_ID('dbo.Fact_Metadatos_Campos', 'U') IS NOT NULL\n        BEGIN\n            DROP TABLE dbo.Fact_Metadatos_Campos;\n        END\n        GO\n\nCREATE TABLE dbo.Fact_Metadatos_Campos (\n    Base VARCHAR(255),     -- Nombre de la base de datos\n    Esquema VARCHAR(128),        -- Nombre del esquema\n    Tabla VARCHAR(255),         -- Nombre de la tabla\n    Campo VARCHAR(128),         -- Nombre del campo\n    Descripcion_Campo VARCHAR(255) NULL,  -- Descripci\u00f3n del campo\n    DataType VARCHAR(128),          -- Tipo de dato\n    Longitud INT NULL,              -- Longitud (para tipos de dato que la requieren)\n    Create_date DATETIME NULL, -- Fecha de creaci\u00f3n\n    Modify_date DATETIME NULL,  -- Fecha de modificaci\u00f3n\n    TotalRegistros INT,\n    Nulos INT,\n    Campo_Repetido VARCHAR(255),\n    Metadatos_Completos VARCHAR(255),\n    Registros_Sin_Exactitud INT\n) ON [PRIMARY];\n\nGO\nIF OBJECT_ID('dbo.Fact_Indicadores_Historicos', 'U') IS NOT NULL\n        BEGIN\n            DROP TABLE dbo.Fact_Indicadores_Historicos;\n        END\n        GO\n\nCREATE TABLE dbo.Fact_Indicadores_Historicos (\n    Id_Registro INT IDENTITY(1,1),     \n    Id_Componente INT,\n    Base VARCHAR(255),     -- Nombre de la base de datos\n    Esquema VARCHAR(128),        -- Nombre del esquema\n    Tabla VARCHAR(255),         -- Nombre de la tabla\n    Fecha_Medida DATETIME DEFAULT GETDATE(),       \n    Valor_Indicador FLOAT\n) ON [PRIMARY];\n\nGO\n</code></pre></p>"},{"location":"02.Tablero/00.Inicio/","title":"Introducci\u00f3n","text":""},{"location":"02.Tablero/00.Inicio/#02-modelo-analitico-de-calidad-de-datos-parametricos-historia-clinica","title":"02. Modelo Anal\u00edtico de Calidad de Datos Param\u00e9tricos Historia Cl\u00ednica","text":"<p>El tablero cuenta con tres partes para su ejecuci\u00f3n, las cuales se encuentran disponibles en los archivos entregados. Es importante tener en cuenta que las fuentes y ETLs del tablero se encuentran en la ruta del repositorio debido al orden de ejecuci\u00f3n que se requiere.</p>"},{"location":"02.Tablero/00.Inicio/#indice","title":"\u00cdndice","text":"<ul> <li>2.1. Fuentes.  </li> <li>2.2. Scripts Python <ul> <li>03.ETLs Tablero Tablas. </li> <li>04.ETLs Tablero Campos. </li> <li>05.ETLs Tablero Indicadores Hist\u00f3ricos. </li> </ul> </li> <li>2.3. Visualizaciones <ul> <li>Visi\u00f3n General Calidad de Datos. </li> <li>An\u00e1lisis de Fuentes de Datos. </li> <li>Componentes Calidad de Datos. </li> </ul> </li> </ul>"},{"location":"02.Tablero/01.Fuentes/","title":"2.1 Fuentes","text":""},{"location":"02.Tablero/01.Fuentes/#21-fuentes","title":"2.1 Fuentes","text":"<p>En la carpeta 01.Repositorio/01.Fuentes se encuentra el archivo Excel 02.Dim_Componente_Calidad.xlsx , que contiene la parametrizaci\u00f3n de los componentes de calidad mostrados en el tablero. Este archivo incluye la descripci\u00f3n de cada componente evaluado, y una serie de acciones generales. Se recomienda no cambiar su configuraci\u00f3n. Cualquier cambio debe contar con la validaci\u00f3n del equipo de Country y una comprensi\u00f3n clara de los impactos en el repositorio y en el tablero. </p> <p></p> <p></p> <p>Vista del archivo 02.Dim_Componente_Calidad.xlsx </p> <p></p>"},{"location":"02.Tablero/Scripts_Python/03.ETLs_Tablero_Tablas/","title":"03.ETLs Tablero Tablas","text":""},{"location":"02.Tablero/Scripts_Python/03.ETLs_Tablero_Tablas/#03-etls-tablero-tablas","title":"03 ETLs Tablero Tablas","text":"<p>El objetivo de este script es generar procesar los metadatos de las tablas y calcular los indicadores de calidad asociados con la estructura necesaria para las visualizaciones del tablero</p>"},{"location":"02.Tablero/Scripts_Python/03.ETLs_Tablero_Tablas/#librerias","title":"Librer\u00edas","text":"<p>Para empezar se importan las librer\u00edas requeridas, las cadenas de conexi\u00f3n connections_start y connections_final, y las funciones db_final y lista_tablas de Funciones Generales</p> <pre><code>import pyodbc\nimport pandas as pd\nimport os\nimport time\nfrom datetime import datetime\nfrom funciones_generales import connections_start,connections_final,db_final,lista_tablas\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/03.ETLs_Tablero_Tablas/#configuracion-para-logs","title":"Configuraci\u00f3n para Logs","text":"<p>En esta parte se identifica si existe la carpeta para guardar los logs. Si no existe la crea autom\u00e1ticamente. Adem\u00e1s, se da inicio al proceso. <pre><code># Directorio para guardar logs (opcional)\noutput_dir = os.path.join(os.getcwd(), \"logs\",\"03.Etls_Tablero_Tablas\")\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n    print(\"Directorio creado:\", output_dir)\n\n\n# Limpiar los archivos de log al inicio del proceso\nopen(os.path.join(output_dir, \"01.log.log\"), \"w\", encoding=\"latin1\").close()\n\n# Definir rutas y abrir los archivos de log (modo append)\nlog_path = os.path.join(output_dir, \"01.log.log\")\nlog = open(log_path, \"a\", encoding=\"latin1\")\n\n# Tiempo de inicio del proceso\nstart_time = time.time()\nprint(\"Inicio del proceso de poblamiento de resultados:\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\nprint(\"Directorio de trabajo:\", os.getcwd())\nlog.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Inicio del proceso de poblamiento de resultados\\n\")\n</code></pre></p>"},{"location":"02.Tablero/Scripts_Python/03.ETLs_Tablero_Tablas/#funciones-para-metadatos-de-tablas","title":"Funciones para metadatos de tablas","text":"<p>Para el procesamiento de los metadatos se definen las siguientes funciones que permiten hacer las consultas a las base del repositorio, es decir a COL_CTY_PARAMETRICA y CTY_PRI_PARAMETRICA.:</p> <pre><code># Consulta el n\u00famero total de registros por tabla y sus duplicados\ndef totales_tabla(esquema,nombre_tabla):\n    # Consultar la tabla\n    query = f\"SELECT * FROM {esquema}.{nombre_tabla};\"\n\n    # Ejecutar la consulta\n    cursor.execute(query)\n    # Obtener los resultados de la consulta como lista de listas\n    rows = [list(row) for row in cursor.fetchall()]  # Convertir cada tupla en lista\n    columns = [column[0] for column in cursor.description]  # Obtener nombres de las columnas\n    # Crear el DataFrame con los resultados\n    df = pd.DataFrame(rows, columns=columns)\n    # Calcular n\u00famero total de registros\n    total_registros = len(df)\n\n    # Identificar duplicados\n    duplicados = df[df.duplicated()]  # Filtra los duplicados\n    numero_duplicados = len(duplicados)\n\n    # Crear un nuevo DataFrame con la informaci\u00f3n\n    resumen_df = pd.DataFrame({\n        'Base' : [db_name],\n        'Esquema' :[esquema],\n        'Tabla' : [nombre_tabla],\n        'TotalRegistros': [total_registros],\n        'Duplicados': [numero_duplicados]\n    })\n    return resumen_df\n</code></pre> <pre><code># Consulta los metadatos de las tablas guardados en la tabla Metadata_Tablas\ndef consulta_metadatos_tablas(db_name,nombre_tabla):\n    # Consultar metadatos de las tablas\n    query_metadatos = f\"\"\"SELECT * FROM {db_name}.[dbo].[Metadata_Tablas] t\n    where t.Tabla = '{nombre_tabla}'\n    ORDER BY t.Tabla\n    \"\"\"\n    # Ejecutar la consulta\n    cursor.execute(query_metadatos)\n    # Obtener los resultados de la consulta como lista de listas\n    rows = [list(row) for row in cursor.fetchall()]  # Convertir cada tupla en lista\n    columns = [column[0] for column in cursor.description]  # Obtener nombres de las columnas\n    # Crear el DataFrame con los resultados\n    df = pd.DataFrame(rows, columns=columns)\n    return df\n</code></pre> <pre><code># Consulta la frecuencia y fecha de actualizacion por tabla\ndef datos_actualizacion_tablas(db_name_final,nombre_tabla):\n    # Consultar metadatos asociados\n    query_metadatos = f\"\"\"SELECT DISTINCT\n       [Base]\n      ,[Esquema]\n      ,[Tabla]\n      ,[FrecuenciaActualizacion]\n      ,[FechaActualizacion]\n    FROM [{db_name_final}].[dbo].[Metadata_Tablas]\n    where Tabla = '{nombre_tabla}'\n    \"\"\"\n    # Ejecutar la consulta\n    cursor_final.execute(query_metadatos)\n    # Obtener los resultados de la consulta como lista de listas\n    rows = [list(row) for row in cursor_final.fetchall()]  # Convertir cada tupla en lista\n    columns = [column[0] for column in cursor_final.description]  # Obtener nombres de las columnas\n    # Crear el DataFrame con los resultados\n    df = pd.DataFrame(rows, columns=columns)\n    return df\n</code></pre> <pre><code># Calcula la fecha de la \u00faltima actualizaci\u00f3n requerida\ndef calcular_ultima_actualizacion(fecha_actualizacion):\n    try:\n        # Obtener la fecha actual\n        fecha_actual = datetime.now()\n        # Convertir \"01-FEBRERO\" en una fecha completa\n        dia, mes_texto = fecha_actualizacion.split('-')\n        meses = {\n            'ENERO': 1, 'FEBRERO': 2, 'MARZO': 3, 'ABRIL': 4, 'MAYO': 5, 'JUNIO': 6,\n            'JULIO': 7, 'AGOSTO': 8, 'SEPTIEMBRE': 9, 'OCTUBRE': 10, 'NOVIEMBRE': 11, 'DICIEMBRE': 12\n        }\n        mes = meses[mes_texto.upper()]\n\n        # Fecha del 1 de febrero en el a\u00f1o actual\n        fecha_actualizacion = datetime(fecha_actual.year, mes, int(dia))\n\n        # Comparar fechas\n        if fecha_actual &gt;= fecha_actualizacion:\n            # Usar el a\u00f1o actual\n            return fecha_actualizacion\n        else:\n            # Usar el a\u00f1o anterior\n            return fecha_actualizacion.replace(year=fecha_actual.year - 1)\n    except:\n        # Fecha del 1 de febrero en el a\u00f1o actual\n        fecha_actualizacion = datetime(fecha_actual.year, 2, 1)\n\n        # Comparar fechas\n        if fecha_actual &gt;= fecha_actualizacion:\n            # Usar el a\u00f1o actual\n            return fecha_actualizacion\n        else:\n            # Usar el a\u00f1o anterior\n            return fecha_actualizacion.replace(year=fecha_actual.year - 1)\n</code></pre> <pre><code># Calcula si la \u00faltima actualizaci\u00f3n realizada se encuentra vigente de acuerdo con la fecha de la \u00faltima actualizaci\u00f3n requerida\ndef es_actualizacion_vigente(row):\n    if pd.isnull(row['last_user_update']) or pd.isnull(row['Ultima_Actualizacion_Requerida']):\n        return 'NO'  # Considerar no vigente si hay valores nulos\n    elif row['last_user_update'] is None:\n        return 'NO'    \n    return 'SI' if row['last_user_update'] &gt;= row['Ultima_Actualizacion_Requerida'] else 'NO'\n</code></pre> <pre><code># Definici\u00f3n de los grupos de las tablas\ndict_descripcion = {\n    \"ACF\" : \"M\u00f3dulo de Activos Fijo\",\n    \"ADP\" : \"M\u00f3dulo de Nomina en SAHI\",\n    \"ASI\" : \"Modulo del Administrador del sistema\",\n    \"ADM\" : \"Todo lo que tiene que ver con parte administrativo\",\n    \"HCE\" : \"Todo lo que tiene que ver con temas asistenciales\",\n    \"GEN\" : \"Todo lo relacionado con datos maestros\" \n}\n</code></pre> <pre><code># Calcula los metadatos de las tablas con la estructura para el tablero\ndef fact_metadatos_tabla(db_name,esquema,nombre_tabla,df_total):\n    # Consultar metadatos disponibles\n    df_metadatos = consulta_metadatos_tablas(db_name,nombre_tabla)\n    # Identificar si los metadatos est\u00e1n completos\n    df_metadatos['Metadatos_Completos'] = df_metadatos.isnull().any(axis=1).apply(lambda x: 'NO' if x else 'SI')\n    df_procesado = pd.merge(df_total,df_metadatos,how='left', on =['Base','Esquema','Tabla'])\n    # Incluir informaci\u00f3n de grupos\n    df_procesado['Grupo'] = nombre_tabla[:3]\n    df_procesado['Descripcion_Grupo'] = df_procesado['Grupo'].apply(\n        lambda grupo: dict_descripcion.get(grupo, \"Grupo sin descripci\u00f3n asignada\")\n    )\n\n    df_tabla = df_procesado.copy()\n    # Ajustar Fecha de actualizacion\n    df_tabla['FechaActualizacion'] = df_tabla['FechaActualizacion'].fillna('01-FEBRERO')\n    df_tabla['FechaActualizacion'] = df_tabla['FechaActualizacion'].astype(str)\n    # Completa valores nulos\n    df_tabla['Metadatos_Completos'] = df_tabla['Metadatos_Completos'].fillna('NO')\n    # Ajustar frencuencua de actualizacion \n    df_tabla['FrecuenciaActualizacion'] = df_tabla['FrecuenciaActualizacion'].fillna('ANUAL')\n    #Calcular la fecha de la ultima actualizacion requerida\n    df_tabla['Ultima_Actualizacion_Requerida'] = df_tabla['FechaActualizacion'].apply(calcular_ultima_actualizacion)\n\n    # Calcular si la ultima actualizacion esta vigente o no\n    df_tabla['Actualizacion_Vigente'] = df_tabla.apply(es_actualizacion_vigente, axis=1)\n\n    # Ordenar el dataframe por 'last_user_update' en orden descendente\n    df_tabla = df_tabla.sort_values(by='last_user_update', ascending=False)\n    df_tabla = df_tabla.drop_duplicates(subset=['Tabla']).copy()\n    return df_tabla\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/03.ETLs_Tablero_Tablas/#proceso-principal","title":"Proceso principal","text":"<p>Se define la conexi\u00f3n a las bases de origen con las cadenas de conexi\u00f3n definidas en connections_start y se ejecuta el siguiente bloque:</p> <pre><code># Conectar a las bases y poblar tablas\nesquema = 'dbo'\nfor db_name, conn_str in connections_start.items():\n    print(f\"Procesando base de datos: {db_name}\")\n    log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Procesando base de datos: {db_name}\\n\")\n    #print(conn_str)\n    try:\n        # Conexion a base fuente\n        conn = pyodbc.connect(conn_str)\n        cursor = conn.cursor()\n\n        #Conexion a base destino\n\n        db_name_final = db_final(db_name)\n        conn_str_final = connections_final[db_name_final]\n        conn_final = pyodbc.connect(conn_str_final)\n        cursor_final = conn_final.cursor()\n        print(\"Intentando conexion\")\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Conexi\u00f3n a {db_name_final} exitosa \\n\")\n        print(\"Conexion Hecha\")\n\n        try:\n            # Limpiar la tabla Fact_Metadatos_Tablas\n            clean_query = f\"DELETE FROM dbo.Fact_Metadatos_Tablas;\"\n            cursor_final.execute(clean_query)\n            conn_final.commit()\n            print(f\"Tabla dbo.Fact_Metadatos_Tablas limpiada exitosamente\")\n            log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Tabla dbo.Fact_Metadatos_Tablas limpiada exitosamente\\n\")\n        except Exception as e:\n            print(f\"Error al limpiar la tabla dbo.Fact_Metadatos_Tablas: {e}\")\n            log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: Al limpiar la tabla dbo.Fact_Metadatos_Tablas: {e}\\n\")\n\n        # Identificar tablas a incluir\n        tablas_filtradas = lista_tablas(db_name_final,cursor_final)\n        for nombre_tabla in tablas_filtradas:\n            log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Procesando {nombre_tabla} \\n\")\n            # Calcular totales\n            df_total = totales_tabla(esquema,nombre_tabla)\n            # Completar todos los metadatos\n            df = fact_metadatos_tabla(db_name_final,esquema,nombre_tabla,df_total)\n            # Ajustar formtatos de campos de fecha\n            df['create_date'] = df['create_date'].astype('object')\n            df['modify_date'] = df['modify_date'].astype('object')\n            df['last_user_update'] = df['last_user_update'].astype('object')\n            df['Ultima_Actualizacion_Requerida'] = df['Ultima_Actualizacion_Requerida'].astype('object')\n            # Ajustar estructura para subir por medio de comandos SQL\n            for index, row in df.iterrows():\n                # Reemplazar el valor en el DataFrame original con el formato CONVERT\n                # Para 'create_date'\n                if pd.isna(row['create_date']):\n                    df.at[index, 'create_date'] = None\n                else:\n                    df.at[index, 'create_date'] = f\"CONVERT(DATETIME2, '{row['create_date']}', 120)\"     \n                # Para 'modify_date'\n                if pd.isna(row['modify_date']):\n                    df.at[index, 'modify_date'] = None\n                else:\n                    df.at[index, 'modify_date'] = f\"CONVERT(DATETIME2, '{row['modify_date']}', 120)\"      \n\n                # Para 'last_user_update' \n                if pd.isna(row['last_user_update']):\n                    df.at[index, 'last_user_update'] = None\n                else:\n                    df.at[index, 'last_user_update'] = f\"CONVERT(DATETIME2, '{row['last_user_update']}', 120)\"\n\n                # Para 'Ultima_actualizacion_requerida' (manejo con try-except)\n                if pd.isna(row['Ultima_Actualizacion_Requerida']):\n                    df.at[index, 'Ultima_Actualizacion_Requerida'] = None\n                else:\n                    df.at[index, 'Ultima_Actualizacion_Requerida'] = f\"CONVERT(DATETIME, '{row['Ultima_Actualizacion_Requerida']}', 120)\"\n            for index, row in df.iterrows():\n                valores = [\n                    \"NULL\" if pd.isna(valor) else f\"'{valor}'\"\n                    for valor in row\n                ]\n                valores_sin_comillas = [v.strip(\"'\") if v.startswith(\"'CONVERT\") and v.endswith(\")'\") else v for v in valores]       \n                columnas = ', '.join([f'{col}' for col in df.columns])\n                try:\n                    query = f\"INSERT INTO dbo.Fact_Metadatos_Tablas ({columnas}) VALUES ({', '.join(valores_sin_comillas)})\"\n                    cursor_final.execute(query)\n                    conn_final.commit()\n                except Exception as e:\n                    print(f\"Error en la estructura de archivo de metadatos {esquema}.{nombre_tabla}: {e}\")\n                    log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: En la estructura de archivo de metadatos {esquema}.{nombre_tabla}: {e}\\n\")\n\n    except Exception as e:\n        print(f\"Error procesando la base de datos {db_name}: {e}\")\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: Procesando la base de datos {db_name}: {e}\\n\")\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/03.ETLs_Tablero_Tablas/#fin-del-proceso","title":"Fin del proceso","text":"<p>Finalmente, se registra el fin del proceso y se guarda el log.</p> <pre><code>#tiempo total\nend_time = time.time()\ntotal_duration = end_time - start_time\nprint(\"Tiempo total de ejecuci\u00f3n:\", time.strftime(\"%H:%M:%S\", time.gmtime(total_duration)))\nlog.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Tiempo total de ejecuci\u00f3n: {time.strftime('%H:%M:%S', time.gmtime(total_duration))}\\n\")\n\n\n#Cerrar log\nlog.close()\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/03.ETLs_Tablero_Tablas/#tiempos-de-procesamiento","title":"Tiempos de procesamiento","text":"<p>Con un equipo con 32 GB de RAM y procesador 11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz 1.69 GHz, el tiempo de ejecuci\u00f3n promedio es de 1 minuto.</p>"},{"location":"02.Tablero/Scripts_Python/04.ETLs_Tablero_Campos/","title":"04.ETLs Tablero Campos","text":""},{"location":"02.Tablero/Scripts_Python/04.ETLs_Tablero_Campos/#04etls-tablero-campos","title":"04.ETLs Tablero Campos","text":"<p>El objetivo de este script es generar procesar los metadatos de los campos y calcular los indicadores de calidad asociados con la estructura necesaria para las visualizaciones del tablero</p>"},{"location":"02.Tablero/Scripts_Python/04.ETLs_Tablero_Campos/#librerias","title":"Librer\u00edas","text":"<p>Para empezar se importan las librer\u00edas requeridas, las cadenas de conexi\u00f3n connections_start y connections_final, y las funciones db_final y lista_tablas de Funciones Generales <pre><code>import pyodbc\nimport pandas as pd\nimport os\nimport time\nimport decimal\nimport numpy as np\nfrom datetime import datetime, date, time as datetime_time\nfrom funciones_generales import connections_start,connections_final,db_final,lista_tablas\npd.set_option('future.no_silent_downcasting', True)\n</code></pre></p>"},{"location":"02.Tablero/Scripts_Python/04.ETLs_Tablero_Campos/#configuracion-para-logs","title":"Configuraci\u00f3n para Logs","text":"<p>En esta parte se identifica si existe la carpeta para guardar los logs. Si no existe la crea autom\u00e1ticamente. Adem\u00e1s, se da inicio al proceso. <pre><code># Directorio para guardar logs (opcional)\noutput_dir = os.path.join(os.getcwd(), \"logs\",\"04.Etls_Tablero_Campos\")\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n    print(\"Directorio creado:\", output_dir)\n\n# Limpiar los archivos de log al inicio del proceso\nopen(os.path.join(output_dir, \"01.log.log\"), \"w\", encoding=\"latin1\").close()\n\n# Definir rutas y abrir los archivos de log (modo append)\nlog_path = os.path.join(output_dir, \"01.log.log\")\nlog = open(log_path, \"a\", encoding=\"latin1\")\n\n\n# Tiempo de inicio del proceso\nstart_time = time.time()\nprint(\"Inicio del proceso de poblamiento de resultados:\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\nprint(\"Directorio de trabajo:\", os.getcwd())\nlog.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Inicio del proceso de poblamiento de resultados\\n\")\n</code></pre></p>"},{"location":"02.Tablero/Scripts_Python/04.ETLs_Tablero_Campos/#funciones-para-metadatos-de-campos","title":"Funciones para metadatos de campos","text":"<p>Para el procesamiento de los metadatos se definen las siguientes funciones que permiten hacer las consultas a las base del repositorio, es decir a COL_CTY_PARAMETRICA y CTY_PRI_PARAMETRICA.:</p> <pre><code># Diccionario con la analog\u00eda entre tipos de datos de SQL Server y Python\ntipos_datos = {\n    'bigint': 'int',\n    'bit': 'bool',\n    'char': 'str',\n    'date': 'date',\n    'datetime': 'datetime',\n    'datetime2': 'datetime',\n    'decimal': 'decimal.Decimal',\n    'float': 'float',\n    'int': 'int',\n    'money': 'decimal.Decimal',\n    'numeric': 'decimal.Decimal',\n    'nvarchar': 'str',\n    'real': 'float',\n    'smalldatetime': 'datetime',\n    'smallint': 'int',\n    'text': 'str',\n    'time': 'datetime_time',\n    'tinyint': 'int',\n    'varbinary': 'bytes',\n    'varchar': 'str',\n    'image' :'image'\n}\n</code></pre> <pre><code># Consulta los metadatos de los campos guardados en la tabla Metadata_Campos\ndef consulta_metadatos_campos(db_name,nombre_tabla):\n    # Consultar metadatos de los campos\n    query_metadatos = f\"\"\"SELECT * FROM {db_name}.[dbo].[Metadata_Campos] t\n    where t.Tabla = '{nombre_tabla}'\n    ORDER BY t.Tabla\n    \"\"\"\n    # Ejecutar la consulta\n    cursor.execute(query_metadatos)\n    # Obtener los resultados de la consulta como lista de listas\n    rows = [list(row) for row in cursor.fetchall()]  # Convertir cada tupla en lista\n    columns = [column[0] for column in cursor.description]  # Obtener nombres de las columnas\n    # Crear el DataFrame con los resultados\n    df = pd.DataFrame(rows, columns=columns)\n    return df\n</code></pre> <pre><code># Consulta los campos con nombres repetidos\ndef lista_campos_repetidos(db_name_final):\n    query_fields_list = f\"\"\"SELECT \n            TABLE_CATALOG AS Base,\n            TABLE_SCHEMA AS Esquema,\n            TABLE_NAME AS Tabla,\n            COLUMN_NAME AS Columna\n        FROM  {db_name_final}.INFORMATION_SCHEMA.COLUMNS\n        WHERE \n            TABLE_CATALOG = 'COL_CTY_PARAMETRICA'\n            AND TABLE_SCHEMA = 'dbo'\n            AND TABLE_NAME NOT IN ('Metadata_Tablas','Metadata_Campos','Metadata_Relaciones','Fact_Metadatos_Tablas','Fact_Metadatos_Campos');\"\"\"\n    # Ejecutar la consulta\n    cursor.execute(query_fields_list)\n\n    # Obtener los resultados de la consulta como lista de listas\n    rows = [list(row) for row in cursor.fetchall()]  # Convertir cada tupla en lista\n    columns = [column[0] for column in cursor.description]  # Obtener nombres de las columnas\n    # Crear el DataFrame con los resultados\n    df_campos = pd.DataFrame(rows, columns=columns)\n\n    # Contar las ocurrencias de cada valor en la columna \"Columna\"\n    conteo = df_campos['Columna'].value_counts()\n    # Filtrar las columnas que aparecen m\u00e1s de una vez\n    columnas_repetidas = conteo[conteo &gt; 1].index\n    # Crear un dataframe con las columnas repetidas y sus conteos\n    df_campos_repetidos = pd.DataFrame({\n        'Campo': columnas_repetidas,\n        #'Frecuencia': conteo[conteo &gt; 1].values\n        'Campo_Repetido': 'SI'\n    })\n    df_campos_repetidos\n    return df_campos_repetidos\n</code></pre> <pre><code># Calcula los metadatos de los campos con la estructura para el tablero\ndef fact_metadatos_campos(db_name_final,nombre_tabla):\n    #Consultar Metadatos de los campos e identificar si estan completos\n    df_metadatos_campos = consulta_metadatos_campos(db_name_final,nombre_tabla)\n    df_metadatos_campos['Metadatos_Completos'] = df_metadatos_campos.isnull().any(axis=1).apply(lambda x: 'NO' if x else 'SI')\n\n    # Consulta la tabla\n    query = f\"SELECT * FROM {db_name}.{esquema}.{nombre_tabla};\"\n\n    # Ejecuta la consulta\n    cursor.execute(query)\n\n    # Obtener los resultados de la consulta como lista de listas\n    rows = [list(row) for row in cursor.fetchall()]  # Convertir cada tupla en lista\n    columns = [column[0] for column in cursor.description]  # Obtener nombres de las columnas\n\n    # Crear el DataFrame con los resultados\n    df = pd.DataFrame(rows, columns=columns)\n\n    # Reemplazar espacios en blanco por NaN\n    df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n\n    # Contar los registros no nulos por columna\n    conteo_por_columna = df.count()\n\n    # Convertir el conteo a un dataframe\n    df_conteo = conteo_por_columna.reset_index()\n    df_conteo.columns = ['Campo', 'No_Nulos']  # Renombrar las columnas para mayor claridad\n    df_conteo['TotalRegistros'] = len(df)\n    df_conteo['Base'] = db_name\n    df_conteo['Esquema'] = esquema\n    df_conteo['Tabla'] = nombre_tabla\n    df_conteo['Nulos'] = df_conteo['TotalRegistros'] - df_conteo['No_Nulos']\n\n    #Incluir informacion de los campos repetidos\n    df_campos_repetidos = lista_campos_repetidos(db_name_final)\n    df_totales = pd.merge(df_conteo,df_campos_repetidos, how ='left', on = 'Campo')\n    df_totales['Campo_Repetido'] = df_totales['Campo_Repetido'].fillna('NO')\n\n    df_totales = df_totales[['Base', 'Esquema', 'Tabla','Campo', 'TotalRegistros', 'Nulos','Campo_Repetido']].copy()\n\n    ##Unir ambos dataframes\n    df_metadatos = pd.merge(df_totales,df_metadatos_campos, how ='left', on = ['Base', 'Esquema', 'Tabla','Campo'] )\n    ## Regla: Si los campos no tienen metadatos para el tipo de campo se consideran como varchar\n    df_metadatos['DataType'] = df_metadatos['DataType'].fillna('varchar')\n\n\n    lista_campos = list(df_metadatos[['Campo', 'DataType']].itertuples(index=False, name=None))\n    df_tipo = []\n    for campo,datatype in lista_campos:\n        tipo = tipos_datos[datatype]\n        # Verificar si todos los valores en la columna son del tipo de dato definido\n        val_tipo = df[campo].apply(lambda x: True if pd.isnull(x) else (True if tipo == 'image' else isinstance(x, eval(tipo))))\n\n        # Contar los valores que son False en val_tipo\n        conteo_false = (~val_tipo).sum()\n        df_tipo.append(conteo_false)\n\n    df_metadatos['Registros_Sin_Exactitud'] = df_tipo\n    df_metadatos['Longitud'] = pd.to_numeric(df_metadatos['Longitud'], errors='coerce').astype('Int64')\n    df_metadatos['Metadatos_Completos'] = df_metadatos['Metadatos_Completos'].fillna('NO')\n    df_metadatos\n\n    return df_metadatos\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/04.ETLs_Tablero_Campos/#proceso-principal","title":"Proceso principal","text":"<p>Se define la conexi\u00f3n a las bases de origen con las cadenas de conexi\u00f3n definidas en connections_start y se ejecuta el siguiente bloque:</p> <pre><code># Conectar a las bases y poblar tablas\nesquema = 'dbo'\nfor db_name, conn_str in connections_start.items():\n    print(f\"Procesando base de datos: {db_name}\")\n    log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Procesando base de datos: {db_name}\\n\")\n    try:\n        # Conexion a base fuente\n        conn = pyodbc.connect(conn_str)\n        cursor = conn.cursor()\n\n        #Conexion a base destino\n        db_name_final = db_final(db_name)\n        conn_str_final = connections_final[db_name_final]\n        conn_final = pyodbc.connect(conn_str_final)\n        cursor_final = conn_final.cursor()\n        print(\"Intentando conexion\")\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Conexi\u00f3n a {db_name_final} exitosa \\n\")\n        print(\"Conexion Hecha\")\n\n        try:\n            # Limpiar la tabla Fact_Metadatos_Campos\n            clean_query = f\"DELETE FROM dbo.Fact_Metadatos_Campos;\"\n            cursor_final.execute(clean_query)\n            conn_final.commit()\n            print(f\"Tabla dbo.Fact_Metadatos_Campos limpiada exitosamente\")\n            log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Tabla dbo.Fact_Metadatos_Campos limpiada exitosamente\\n\")\n        except Exception as e:\n            print(f\"Error al limpiar la tabla dbo.Fact_Metadatos_Campos: {e}\")\n            log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: Al limpiar la tabla dbo.Fact_Metadatos_Campos: {e}\\n\")\n\n        # Identificar tablas a incluir\n        tablas_filtradas = lista_tablas(db_name_final,cursor_final)\n        for nombre_tabla in tablas_filtradas:\n            log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Procesando {nombre_tabla} \\n\")\n            # Consultar todos los metadatos\n            df = fact_metadatos_campos(db_name_final,nombre_tabla)\n            # Ajustar formtatos de campos de fecha\n            df['Create_date'] = df['Create_date'].astype('object')\n            df['Modify_date'] = df['Modify_date'].astype('object')\n            # Ajustar estructura para subir por medio de comandos SQL\n            for index, row in df.iterrows():\n                # Reemplazar el valor en el DataFrame original con el formato CONVERT\n                # Para 'create_date'\n                if pd.isna(row['Create_date']):\n                    df.at[index, 'Create_date'] = None\n                else:\n                    df.at[index, 'Create_date'] = f\"CONVERT(DATETIME2, '{row['Create_date']}', 120)\"     \n                # Para 'modify_date'\n                if pd.isna(row['Modify_date']):\n                    df.at[index, 'Modify_date'] = None\n                else:\n                    df.at[index, 'Modify_date'] = f\"CONVERT(DATETIME2, '{row['Modify_date']}', 120)\"      \n\n            for index, row in df.iterrows():\n                valores = [\n                    \"NULL\" if pd.isna(valor) else f\"'{valor}'\"\n                    for valor in row\n                ]\n                valores_sin_comillas = [v.strip(\"'\") if v.startswith(\"'CONVERT\") and v.endswith(\")'\") else v for v in valores]       \n                columnas = ', '.join([f'{col}' for col in df.columns])\n\n                try:\n                    query = f\"INSERT INTO dbo.Fact_Metadatos_Campos ({columnas}) VALUES ({', '.join(valores_sin_comillas)})\"\n                    cursor_final.execute(query)\n                    conn_final.commit()\n                except Exception as e:\n                    print(f\"Error en la estructura de archivo de metadatos {esquema}.{nombre_tabla}: {e}\")\n                    log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: En la estructura de archivo de metadatos {esquema}.{nombre_tabla}: {e}\\n\")\n                    continue\n\n\n\n    except Exception as e:\n        print(f\"Error procesando la base de datos {db_name}: {e}\")\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: Procesando la base de datos {db_name}: {e}\\n\")\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/04.ETLs_Tablero_Campos/#fin-del-proceso","title":"Fin del proceso","text":"<p>Finalmente, se registra el fin del proceso y se guarda el log.</p> <pre><code>#tiempo total\nend_time = time.time()\ntotal_duration = end_time - start_time\nprint(\"Tiempo total de ejecuci\u00f3n:\", time.strftime(\"%H:%M:%S\", time.gmtime(total_duration)))\nlog.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Tiempo total de ejecuci\u00f3n: {time.strftime('%H:%M:%S', time.gmtime(total_duration))}\\n\")\n\n#Cerrar log\nlog.close()\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/04.ETLs_Tablero_Campos/#tiempos-de-procesamiento","title":"Tiempos de procesamiento","text":"<p>Con un equipo con 32 GB de RAM y procesador 11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz 1.69 GHz, el tiempo de ejecuci\u00f3n promedio es de 4 minutos.</p>"},{"location":"02.Tablero/Scripts_Python/05.ETLs_Tablero_Indicadores_Historicos/","title":"05.ETLs Tablero Indicadores Hist\u00f3ricos","text":""},{"location":"02.Tablero/Scripts_Python/05.ETLs_Tablero_Indicadores_Historicos/#05-etls-tablero-indicadores-historicos","title":"05 ETLs Tablero Indicadores Hist\u00f3ricos","text":"<p>El objetivo de este script es calcular y guardar los indicadores de calidad a nivel de tabla para contar con el hist\u00f3rico de la informaci\u00f3n.</p>"},{"location":"02.Tablero/Scripts_Python/05.ETLs_Tablero_Indicadores_Historicos/#librerias","title":"Librer\u00edas","text":"<p>Para empezar se importan las librer\u00edas requeridas, las cadenas de conexi\u00f3n connections_start y connections_final, y las funciones db_final y lista_tablas de Funciones Generales</p> <pre><code>import pyodbc\nimport pandas as pd\nimport os\nimport time\nimport decimal\nimport numpy as np\nfrom datetime import datetime, date, time as datetime_time\nfrom funciones_generales import connections_start,connections_final,db_final,lista_tablas\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/05.ETLs_Tablero_Indicadores_Historicos/#configuracion-para-logs","title":"Configuraci\u00f3n para Logs","text":"<p>En esta parte se identifica si existe la carpeta para guardar los logs. Si no existe la crea autom\u00e1ticamente. Adem\u00e1s, se da inicio al proceso.</p> <pre><code># Directorio para guardar logs (opcional)\noutput_dir = os.path.join(os.getcwd(), \"logs\",\"05.Etls_Tablero_Indicadores\")\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n    print(\"Directorio creado:\", output_dir)\n\n# Limpiar los archivos de log al inicio del proceso\nopen(os.path.join(output_dir, \"01.log.log\"), \"w\", encoding=\"latin1\").close()\n\n# Definir rutas y abrir los archivos de log (modo append)\nlog_path = os.path.join(output_dir, \"01.log.log\")\nlog = open(log_path, \"a\", encoding=\"latin1\")\n\n\n# Definir la carpeta de donde esta la lista de fuentes en Excel a incorporar\nCURRENT_DIR = os.getcwd()\nPARENT_DIR = os.path.dirname(CURRENT_DIR)\n\n\n# Tiempo de inicio del proceso\nstart_time = time.time()\nprint(\"Inicio del proceso de poblamiento de resultados:\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\nprint(\"Directorio de trabajo:\", os.getcwd())\nlog.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Inicio del proceso de poblamiento de resultados\\n\")\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/05.ETLs_Tablero_Indicadores_Historicos/#configuracion-para-dim-componente-calidad","title":"Configuraci\u00f3n para Dim Componente Calidad","text":"<p>Para la tabla Dim_Componente_Calidad se cuenta con el archivo Excel 02.Dim_Componente_Calidad.xlsx explicado en la secci\u00f3n de fuentes del tablero. De esta manera, el siguiente bloque de c\u00f3digo lee el archivo y lo guarda en el repositorio.</p> <pre><code># Definir ruta de carpeta con parametrica Dim_Componente_Calidad\nruta_excel = os.path.join(PARENT_DIR , \"01.Fuentes\",\"02.Dim_Componente_Calidad.xlsx\")\n# Leer el archivo Excel\ndf = pd.read_excel(ruta_excel)\n\n# Guardar Dim_Componente_Calidad\nesquema = 'dbo'\nfor db_name, conn_str in connections_start.items():\n    print(f\"Procesando base de datos: {db_name}\")\n    log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Procesando base de datos: {db_name}\\n\")\n    #print(conn_str)\n    try:\n        # Conexion a base fuente\n        conn = pyodbc.connect(conn_str)\n        cursor = conn.cursor()\n\n        #Conexion a base destino\n\n        db_name_final = db_final(db_name)\n        conn_str_final = connections_final[db_name_final]\n        conn_final = pyodbc.connect(conn_str_final)\n        cursor_final = conn_final.cursor()\n        print(\"Intentando conexion\")\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Conexi\u00f3n a {db_name_final} exitosa \\n\")\n        print(\"Conexion Hecha\")\n\n        try:\n            # Limpiar la tabla Dim_Componente_Calidad\n            clean_query = f\"DELETE FROM dbo.Dim_Componente_Calidad;\"\n            cursor_final.execute(clean_query)\n            conn_final.commit()\n            print(f\"Tabla dbo.Dim_Componente_Calidad limpiada exitosamente\")\n            log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Tabla dbo.Dim_Componente_Calidad limpiada exitosamente\\n\")\n        except Exception as e:\n            print(f\"Error al limpiar la tabla dbo.Dim_Componente_Calidad: {e}\")\n            log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: Al limpiar la tabla dbo.Dim_Componente_Calidad: {e}\\n\")\n\n        try:\n            log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Procesando Tabla dbo.Dim_Componente_Calidad \\n\")                  \n            for index, row in df.iterrows():\n                valores = [\n                    \"NULL\" if pd.isna(valor) else f\"'{valor}'\"\n                    for valor in row\n                ]\n                valores_sin_comillas = [v.strip(\"'\") if v.startswith(\"'CONVERT\") and v.endswith(\")'\") else v for v in valores]       \n                columnas = ', '.join([f'{col}' for col in df.columns])\n                try:\n                    query = f\"INSERT INTO dbo.Dim_Componente_Calidad ({columnas}) VALUES ({', '.join(valores_sin_comillas)})\"\n                    cursor_final.execute(query)\n                    conn_final.commit()\n                except Exception as e:\n                    print(f\"Error al subir archivo de dbo.Dim_Componente_Calidad: {e}\")\n                    log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: Error al subir archivo de dbo.Dim_Componente_Calidad: {e}\\n\")\n                    continue\n        except Exception as e:\n            print(f\"Error en la estructura de archivo de dbo.Dim_Componente_Calidad: {e}\")\n            log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: En la estructura de archivo de dbo.Dim_Componente_Calidad: {e}\\n\")\n        conn.close()\n        conn_final.close()            \n\n    except Exception as e:\n        print(f\"Error procesando la base de datos {db_name}: {e}\")\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: Procesando la base de datos {db_name}: {e}\\n\")\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/05.ETLs_Tablero_Indicadores_Historicos/#funciones-para-calculo-de-indicadores-a-nivel-de-tabla","title":"Funciones para c\u00e1lculo de indicadores a nivel de tabla","text":"<p>Para el c\u00e1lculo de los indicadores a nivel de tabla se definen las siguientes funciones:</p> <pre><code># Consultas los metadatos de los campos con la estructura necesaria para el tablero\ndef consulta_fact_metadatos_campos(db_name_final):\n    # Consultar metadatos de Fact_Metadatos_Campos\n    query_metadatos = f\"\"\"SELECT * FROM {db_name_final}.[dbo].[Fact_Metadatos_Campos] t\"\"\"\n    # Ejecutar la consulta\n    cursor.execute(query_metadatos)\n    # Obtener los resultados de la consulta como lista de listas\n    rows = [list(row) for row in cursor.fetchall()]  # Convertir cada tupla en lista\n    columns = [column[0] for column in cursor.description]  # Obtener nombres de las columnas\n    # Crear el DataFrame con los resultados\n    df = pd.DataFrame(rows, columns=columns)\n    return df\n</code></pre> <pre><code># Consultas los metadatos de las tablas con la estructura necesaria para el tablero\ndef consulta_fact_metadatos_tablas(db_name_final):\n    # Consultar metadatos de Fact_Metadatos_Campos\n    query_metadatos = f\"\"\"SELECT * FROM {db_name_final}.[dbo].[Fact_Metadatos_Tablas] t\"\"\"\n    # Ejecutar la consulta\n    cursor.execute(query_metadatos)\n    # Obtener los resultados de la consulta como lista de listas\n    rows = [list(row) for row in cursor.fetchall()]  # Convertir cada tupla en lista\n    columns = [column[0] for column in cursor.description]  # Obtener nombres de las columnas\n    # Crear el DataFrame con los resultados\n    df = pd.DataFrame(rows, columns=columns)\n    return df\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/05.ETLs_Tablero_Indicadores_Historicos/#funcion-exactitud","title":"Funci\u00f3n Exactitud","text":"<pre><code># Calcula el indicador de exactitud\ndef exactitud_tabla(db_name_final):\n    # Consultar la tabla de fact metadatos campos\n    df = consulta_fact_metadatos_campos(db_name_final)\n\n    # Agregar id del indicador y calcularlo\n    df['Id_Componente'] = 1\n    df['Valor_Indicador'] = 1-df['Registros_Sin_Exactitud']/df['TotalRegistros']\n    df['Valor_Indicador'] = df['Valor_Indicador'].fillna(1)\n\n    #Seleccionar las columnas para agrupar\n    df_exactitud = df[['Id_Componente','Base','Esquema','Tabla','Valor_Indicador']].copy()\n\n    # Agrupar por las columnas 'Id_Componente','Base', 'Esquema', 'Tabla',  y calcular el promedio\n    df_promedio = df_exactitud.groupby(['Id_Componente','Base', 'Esquema', 'Tabla'], as_index=False).mean()\n    return df_promedio\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/05.ETLs_Tablero_Indicadores_Historicos/#funcion-completitud","title":"Funci\u00f3n Completitud","text":"<pre><code># Calcula el indicador de completitud\ndef completitud_tabla(db_name_final):\n    # Consultar la tabla de fact metadatos campos\n    df = consulta_fact_metadatos_campos(db_name_final)\n\n    # Agregar id del indicador y calcularlo\n    df['Id_Componente'] = 2\n    df['Valor_Indicador'] = 1-df['Nulos']/df['TotalRegistros']\n    df['Valor_Indicador'] = df['Valor_Indicador'].fillna(1)\n\n    #Seleccionar las columnas para agrupar\n    df_exactitud = df[['Id_Componente','Base','Esquema','Tabla','Valor_Indicador']].copy()\n\n    # Agrupar por las columnas 'Id_Componente','Base', 'Esquema', 'Tabla',  y calcular el promedio\n    df_promedio = df_exactitud.groupby(['Id_Componente','Base', 'Esquema', 'Tabla'], as_index=False).mean()\n    return df_promedio\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/05.ETLs_Tablero_Indicadores_Historicos/#funcion-consistencia","title":"Funci\u00f3n Consistencia","text":"<pre><code># Calcula el indicador de consistencia\ndef consistencia_tabla(db_name_final):\n    # Consultar la tabla de fact metadatos campos\n    df = consulta_fact_metadatos_campos(db_name_final)\n\n    #Seleccionar las columnas para agrupar\n    df_v1_total = df[['Base','Esquema','Tabla','TotalRegistros']].copy()\n    # Agrupar por las columnas 'Base', 'Esquema', 'Tabla',  y calcular la suma\n    df_v1_total_sum = df_v1_total.groupby(['Base', 'Esquema', 'Tabla'], as_index=False).sum()\n\n    #Seleccionar las columnas repetidas\n    df_v1_repetidos_inicial = df[df['Campo_Repetido'] == 'SI'].copy()\n    #Seleccionar las columnas para agrupar\n    df_v1_repetidos = df_v1_repetidos_inicial[['Base','Esquema','Tabla','TotalRegistros']].copy()\n    # Agrupar por las columnas 'Base', 'Esquema', 'Tabla',  y calcular la suma\n    df_v1_repetidos_sum = df_v1_repetidos.groupby(['Base', 'Esquema', 'Tabla'], as_index=False).sum()\n    df_v1_repetidos_sum.rename(columns = {'TotalRegistros' : 'Repetidos'},inplace = True)\n\n    ## Unificar para calcular variable V1 (Columnas) para el indicador de consistencia\n    df_v1 = pd.merge(df_v1_total_sum,df_v1_repetidos_sum, how ='left', on = ['Base','Esquema','Tabla'] )\n    df_v1['Repetidos'] = df_v1['Repetidos'].fillna(0)\n    df_v1['V1_Consistencia'] = 1- df_v1['Repetidos']/df_v1['TotalRegistros'] \n    df_v1['V1_Consistencia'] = df_v1['V1_Consistencia'].fillna(1)\n    df_v1 = df_v1[['Base','Esquema','Tabla','V1_Consistencia']].copy()\n\n    # Consultar metadatos de tablas\n    df_tablas = consulta_fact_metadatos_tablas(db_name_final)\n    df_v2 = df_tablas[['Base','Esquema','Tabla','TotalRegistros','Duplicados']].copy()\n    ## Calcular variable V2 (Filas) para el indicador de consistencia\n    df_v2['V2_Consistencia'] = 1- df_v2['Duplicados']/df_v2['TotalRegistros']\n    df_v2['V2_Consistencia'] = df_v2['V2_Consistencia'].fillna(1)\n    df_v2 = df_v2[['Base','Esquema','Tabla','V2_Consistencia']].copy()\n\n    ## Unificar y calcular consistencia a nivel de tabla\n    df_consistencia = pd.merge(df_v1,df_v2,how ='inner',on = ['Base','Esquema','Tabla'])\n    # Agregar id del indicador y calcularlo\n    df_consistencia['Id_Componente'] = 3\n    df_consistencia['Valor_Indicador'] = (df_consistencia['V1_Consistencia'] + df_consistencia['V2_Consistencia'])*0.5\n    df_consistencia['Valor_Indicador'] = df_consistencia['Valor_Indicador'].fillna(1)\n    #Seleccionar las columnas para agrupar\n    df_consistencia = df_consistencia[['Id_Componente','Base','Esquema','Tabla','Valor_Indicador']].copy()\n    return df_consistencia\n\n ```\n### Funci\u00f3n Actualizaci\u00f3n\n\n```python\n# Calcula el indicador de actualizaci\u00f3n\ndef actualizacion_tabla(db_name_final):\n    # Consultar metadatos de tablas\n    df_tablas = consulta_fact_metadatos_tablas(db_name_final)\n    df_tablas['Valor_Indicador'] = df_tablas['Actualizacion_Vigente'].apply(lambda x: 1 if x =='SI' else 0)\n    df_tablas['Id_Componente'] = 4\n    df_actualizacion = df_tablas[['Id_Componente','Base','Esquema','Tabla','Valor_Indicador']].copy()\n    return df_actualizacion\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/05.ETLs_Tablero_Indicadores_Historicos/#funcion-trazabilidad","title":"Funci\u00f3n Trazabilidad","text":"<pre><code># Calcula el indicador de trazabilidad\ndef trazabilidad_tabla(db_name_final):\n    # Consultar metadatos de tablas\n    df_tablas = consulta_fact_metadatos_tablas(db_name_final)\n    df_tablas_fechas = df_tablas[['Base','Esquema','Tabla','create_date','modify_date','last_user_update']].copy()\n\n    #Identificar si se cuenta con fechas de actualizacion de datos, creacion de tabla y actualizacion de metadatos\n    df_tablas_fechas['Actualizacion'] = df_tablas_fechas['last_user_update'].apply(lambda x: 0 if pd.isnull(x) else 1)\n    df_tablas_fechas['Creacion'] = df_tablas_fechas['create_date'].apply(lambda x: 0 if pd.isnull(x) else 1)\n    df_tablas_fechas['Metadatos'] = df_tablas_fechas['modify_date'].apply(lambda x: 0 if pd.isnull(x) else 1)\n\n    # Calcular Trazabilidad \n    df_tablas_fechas['Trazabilidad_aux'] = df_tablas_fechas['Actualizacion']+df_tablas_fechas['Creacion']+df_tablas_fechas['Metadatos']\n    df_tablas_fechas['Valor_Indicador'] = df_tablas_fechas['Trazabilidad_aux'].apply(lambda x: 1 if x &gt; 2 else 0)\n    df_tablas_fechas['Id_Componente'] = 5\n\n    df_trazabilidad = df_tablas_fechas[['Id_Componente','Base','Esquema','Tabla','Valor_Indicador']].copy()\n    return df_trazabilidad\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/05.ETLs_Tablero_Indicadores_Historicos/#funcion-conformidad","title":"Funci\u00f3n Conformidad","text":"<pre><code># Calcula el indicador de conformidad\ndef conformidad_tabla(db_name_final):\n    # Consultar la tabla de fact metadatos campos \n    df_campos = consulta_fact_metadatos_campos(db_name_final)\n    df_campos = df_campos[['Base', 'Esquema', 'Tabla', 'Campo', 'Descripcion_Campo', 'DataType','Longitud', 'Create_date', 'Modify_date']].copy()\n\n    # Consultar metadatos de tablas\n    df_tablas = consulta_fact_metadatos_tablas(db_name_final)\n    df_tablas_descripcion = df_tablas[['Base','Esquema','Tabla','Descripcion_Tabla']].copy()\n\n    #Unificar metadatos\n    df_conformidad = pd.merge(df_campos,df_tablas_descripcion, how ='left', on= ['Base','Esquema','Tabla'] )\n\n    # Reemplazar espacios en blanco por NaN\n    df_conformidad.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n\n    #Identificar si se cuenta con fechas de actualizacion de datos, creacion de tabla y actualizacion de metadatos\n    df_conformidad['Descripcion_Campo_Conformidad'] = df_conformidad['Descripcion_Campo'].apply(lambda x: 0 if pd.isnull(x) else 1)\n    df_conformidad['DataType_Conformidad'] = df_conformidad['DataType'].apply(lambda x: 0 if pd.isnull(x) else 1)\n    df_conformidad['Longitud_Conformidad'] = df_conformidad['Longitud'].apply(lambda x: 0 if pd.isnull(x) else 1)\n    df_conformidad['Create_date_Conformidad'] = df_conformidad['Create_date'].apply(lambda x: 0 if pd.isnull(x) else 1)\n    df_conformidad['Modify_date_Conformidad'] = df_conformidad['Modify_date'].apply(lambda x: 0 if pd.isnull(x) else 1)\n    df_conformidad['Descripcion_Tabla_Conformidad'] = df_conformidad['Descripcion_Tabla'].apply(lambda x: 0 if pd.isnull(x) else 1)\n    df_conformidad['Base_Conformidad'] = df_conformidad['Base'].apply(lambda x: 0 if pd.isnull(x) else 1)\n    df_conformidad['Esquema_Conformidad'] = df_conformidad['Esquema'].apply(lambda x: 0 if pd.isnull(x) else 1)\n    df_conformidad['Tabla_Conformidad'] = df_conformidad['Tabla'].apply(lambda x: 0 if pd.isnull(x) else 1)\n    df_conformidad['Campo_Conformidad'] = df_conformidad['Campo'].apply(lambda x: 0 if pd.isnull(x) else 1)\n\n    # Calcular Conformidad \n    df_conformidad['Valor_Indicador'] = (df_conformidad['Descripcion_Campo_Conformidad'] + df_conformidad['DataType_Conformidad'] \n                                     + df_conformidad['Longitud_Conformidad']+ df_conformidad['Create_date_Conformidad']\n                                     + df_conformidad['Modify_date_Conformidad'] + df_conformidad['Descripcion_Tabla_Conformidad']\n                                     + df_conformidad['Base_Conformidad'] + df_conformidad['Esquema_Conformidad']\n                                     + df_conformidad['Tabla_Conformidad'] + df_conformidad['Campo_Conformidad'])/10\n    df_conformidad['Id_Componente'] = 6\n    df_conformidad = df_conformidad[['Id_Componente','Base','Esquema','Tabla','Valor_Indicador']].copy()\n\n    # Agrupar por las columnas 'Id_Componente','Base', 'Esquema', 'Tabla',  y calcular el promedio\n    df_promedio = df_conformidad.groupby(['Id_Componente','Base', 'Esquema', 'Tabla'], as_index=False).mean()\n\n    return df_promedio\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/05.ETLs_Tablero_Indicadores_Historicos/#funcion-credibilidad","title":"Funci\u00f3n Credibilidad","text":"<pre><code># Calcula el indicador de credibilidad\ndef credibilidad_tabla(db_name_final):\n    # Consultar metadatos de tablas\n    df_tablas = consulta_fact_metadatos_tablas(db_name_final)\n    df_tablas_contacto = df_tablas[['Base','Esquema','Tabla','DataOwner','ContactEmail']].copy()\n\n    # Reemplazar espacios en blanco por NaN\n    df_tablas_contacto.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n\n    #Identificar si se cuenta con fechas de actualizacion de datos, creacion de tabla y actualizacion de metadatos\n    df_tablas_contacto['Propietario'] = df_tablas_contacto['DataOwner'].apply(lambda x: 0 if pd.isnull(x) else 1)\n    df_tablas_contacto['Correo'] = df_tablas_contacto['ContactEmail'].apply(lambda x: 0 if pd.isnull(x) else 1)\n\n    # Calcular Credibilidad \n    df_tablas_contacto['Credibilidad_aux'] = df_tablas_contacto['Propietario'] + df_tablas_contacto['Correo']\n    df_tablas_contacto['Valor_Indicador'] = df_tablas_contacto['Credibilidad_aux'].apply(lambda x: 1 if x &gt; 1 else 0)\n    df_tablas_contacto['Id_Componente'] = 7\n\n    df_credibilidad = df_tablas_contacto[['Id_Componente','Base','Esquema','Tabla','Valor_Indicador']].copy()\n\n    return df_credibilidad\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/05.ETLs_Tablero_Indicadores_Historicos/#funcion-compresibilidad","title":"Funci\u00f3n Compresibilidad","text":"<pre><code># Calcula el indicador de compresibilidad\ndef compresibilidad_tabla(db_name_final):\n    # Consultar la tabla de fact metadatos campos \n    df_campos = consulta_fact_metadatos_campos(db_name_final)\n    df_campos_inicial = df_campos[['Base', 'Esquema', 'Tabla', 'Campo', 'Descripcion_Campo']].copy()\n\n    # Reemplazar espacios en blanco por NaN\n    df_campos_inicial.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n    df_campos_inicial['Valor_Indicador']  = df_campos_inicial['Descripcion_Campo'].apply(lambda x: 0 if pd.isnull(x) else 1)\n\n    df_campos_inicial['Id_Componente'] = 8\n    df_compresibilidad = df_campos_inicial[['Id_Componente','Base','Esquema','Tabla','Valor_Indicador']].copy()\n\n    # Agrupar por las columnas 'Id_Componente','Base', 'Esquema', 'Tabla',  y calcular el promedio\n    df_promedio = df_compresibilidad.groupby(['Id_Componente','Base', 'Esquema', 'Tabla'], as_index=False).mean()\n\n    return df_promedio\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/05.ETLs_Tablero_Indicadores_Historicos/#funciones-y-estructuras-adicionales","title":"Funciones y estructuras adicionales","text":"<pre><code># Guardar los indicadores ya calculados\ndef guardar_indicador_tabla(db_name_final,indicador):\n    try:\n        #Consultar indicador de acuerdo con su funcion\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Procesando {indicador} para Tabla dbo.Fact_Indicadores_Historicos \\n\")                  \n        #df = exactitud_tabla(db_name_final)\n        df = eval(indicadores[indicador])\n\n        #Llevar registros a estructura para subir a SQL Server\n        for index, row in df.iterrows():\n            valores = [\n                \"NULL\" if pd.isna(valor) else f\"'{valor}'\"\n                for valor in row\n            ]\n            valores_sin_comillas = [v.strip(\"'\") if v.startswith(\"'CONVERT\") and v.endswith(\")'\") else v for v in valores]       \n            columnas = ', '.join([f'{col}' for col in df.columns])\n            try:\n                query = f\"INSERT INTO dbo.Fact_Indicadores_Historicos ({columnas}) VALUES ({', '.join(valores_sin_comillas)})\"\n                cursor_final.execute(query)\n                conn_final.commit()\n            except Exception as e:\n                print(f\"Error SQL al subir {indicador} en dbo.Fact_Indicadores_Historicos: {e}\")\n                log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: Error SQL al subir {indicador} en dbo.Fact_Indicadores_Historicos: {e}\\n\")\n                continue\n        print(f\"{indicador} a nivel de Tabla finalizado\")\n    except Exception as e:\n        print(f\"Error al procesar {indicador} en dbo.Fact_Indicadores_Historicos: {e}\")\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: Al procesar {indicador} en dbo.Fact_Indicadores_Historicos: {e}\\n\")\n</code></pre> <pre><code>#Diccionario de indicadores y funciones\nindicadores ={\n    'Exactitud' : 'exactitud_tabla(db_name_final)',\n    'Completitud' : 'completitud_tabla(db_name_final)',\n    'Consistencia' : 'consistencia_tabla(db_name_final)',\n    'Actualizacion' : 'actualizacion_tabla(db_name_final)',\n    'Trazabilidad': 'trazabilidad_tabla(db_name_final)',\n    'Conformidad' : 'conformidad_tabla(db_name_final)',\n    'Credibilidad' : 'credibilidad_tabla(db_name_final)',\n    'Compresibilidad' : 'compresibilidad_tabla(db_name_final)',\n}\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/05.ETLs_Tablero_Indicadores_Historicos/#proceso-principal-para-indicadores","title":"Proceso principal para indicadores","text":"<p>Se define la conexi\u00f3n a las bases de origen con las cadenas de conexi\u00f3n definidas en connections_start y se ejecuta el siguiente bloque:</p> <pre><code># Conectar a las bases y poblar tablas\nesquema = 'dbo'\nfor db_name, conn_str in connections_start.items():\n    print(f\"Procesando base de datos: {db_name}\")\n    log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Procesando base de datos: {db_name}\\n\")\n    try:\n        # Conexion a base fuente\n        conn = pyodbc.connect(conn_str)\n        cursor = conn.cursor()\n\n        #Conexion a base destino\n\n        db_name_final = db_final(db_name)\n        conn_str_final = connections_final[db_name_final]\n        conn_final = pyodbc.connect(conn_str_final)\n        cursor_final = conn_final.cursor()\n        print(\"Intentando conexion\")\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Conexi\u00f3n a {db_name_final} exitosa \\n\")\n        print(\"Conexion Hecha\")\n\n        #Calcular y guardar indicadores\n        for indicador in indicadores:\n            guardar_indicador_tabla(db_name_final,indicador)      \n\n    except Exception as e:\n        print(f\"Error procesando la base de datos {db_name}: {e}\")\n        log.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - ERROR: Procesando la base de datos {db_name}: {e}\\n\")\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/05.ETLs_Tablero_Indicadores_Historicos/#fin-del-proceso","title":"Fin del proceso","text":"<p>Finalmente, se registra el fin del proceso y se guarda el log.</p> <pre><code>#tiempo total\nend_time = time.time()\ntotal_duration = end_time - start_time\nprint(\"Tiempo total de ejecuci\u00f3n:\", time.strftime(\"%H:%M:%S\", time.gmtime(total_duration)))\nlog.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - INFO: Tiempo total de ejecuci\u00f3n: {time.strftime('%H:%M:%S', time.gmtime(total_duration))}\\n\")\n\n#Cerrar log\nlog.close()\n</code></pre>"},{"location":"02.Tablero/Scripts_Python/05.ETLs_Tablero_Indicadores_Historicos/#tiempos-de-procesamiento","title":"Tiempos de procesamiento","text":"<p>Con un equipo con 32 GB de RAM y procesador 11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz 1.69 GHz, el tiempo de ejecuci\u00f3n promedio es de 30 segundos.</p>"},{"location":"02.Tablero/Visualizaciones/00.MR_Medidas/","title":"Modelo Relacional y Medidas","text":""},{"location":"02.Tablero/Visualizaciones/00.MR_Medidas/#modelo-relacional-y-medidas","title":"Modelo Relacional y Medidas","text":""},{"location":"02.Tablero/Visualizaciones/00.MR_Medidas/#modelo-relacional","title":"Modelo Relacional","text":"<p>El modelo relacional para el tablero cuenta con las siguientes tablas</p>"},{"location":"02.Tablero/Visualizaciones/00.MR_Medidas/#medidas","title":"Medidas","text":"<p>Cada medida calculada se describe brevemente, indicando su objetivo, la l\u00f3gica DAX utilizada y (donde aplica) el formato de salida.</p> <ol> <li># Bases de Datos <ul> <li>Objetivo: Cuantificar el n\u00famero de bases analizadas.  </li> <li>Expresi\u00f3n DAX: <pre><code># Bases de Datos = DISTINCTCOUNT(Dim_Base[Id_Base])\n</code></pre></li> <li>Formato: Entero</li> </ul> </li> <li># Campos <ul> <li>Objetivo: Cuantificar el n\u00famero de campos analizados.  </li> <li>Expresi\u00f3n DAX: <pre><code># Campos = DISTINCTCOUNT(Fact_Metadatos_Campos[Id_Campo])\n</code></pre></li> <li>Formato: Entero</li> </ul> </li> <li># Llaves For\u00e1neas <ul> <li>Objetivo: Cuantificar el n\u00famero llaves for\u00e1neas.  </li> <li>Expresi\u00f3n DAX: <pre><code># Llaves For\u00e1neas = \n    var llaves = DISTINCTCOUNT(Fact_Llaves_Foraneas[Id_Llave_Foranea])\n    return if(ISBLANK(llaves),0,llaves)\n</code></pre></li> <li>Formato: Entero</li> </ul> </li> </ol>"},{"location":"02.Tablero/Visualizaciones/01.Vision_General/","title":"Visi\u00f3n General Calidad de Datos","text":""},{"location":"02.Tablero/Visualizaciones/01.Vision_General/#modelo-analitico-analisis-de-calidad-de-datos-parametricos-historia-clinica","title":"Modelo Anal\u00edtico An\u00e1lisis de Calidad de Datos Param\u00e9tricos Historia Cl\u00ednica","text":""},{"location":"02.Tablero/Visualizaciones/01.Vision_General/#menu-principal","title":"Men\u00fa Principal","text":"<p>Este tablero corresponde a un modelo de diagn\u00f3stico para la situaci\u00f3n a la fecha de corte de la calidad de los datos param\u00e9tricos asociados a la historia cl\u00ednica para la Cl\u00ednica del Country y Colina. Esto a partir de los indicadores Exactitud, Completitud, Consistencia, Actualizaci\u00f3n, Trazabilidad, Conformidad, Credibilidad y Compresibilidad. Para esto se consideran las bases de datos, esquemas y tablas definidas en conjunto entre Country y QCS.</p> <p>En el men\u00fa principal se encuentra la descripci\u00f3n del tablero junto con botones que permiten ir a cada una de las visualizaciones: Visi\u00f3n General Calidad de Datos, An\u00e1lisis de Fuentes de Datos y Componentes Calidad de Datos.</p> <p></p>"},{"location":"02.Tablero/Visualizaciones/01.Vision_General/#vision-general-calidad-de-datos","title":"Visi\u00f3n General Calidad de Datos","text":""},{"location":"02.Tablero/Visualizaciones/01.Vision_General/#objetivo","title":"Objetivo","text":"<p>Presentar el estado de los indicadores de calidad asociados a los datos param\u00e9tricos de la historia cl\u00ednica para la fecha de an\u00e1lisis; Para lo cual se muestran los indicadores asociados al n\u00famero de registros, bases de datos, esquemas, tablas y campos analizados, junto el nivel general de calidad y los componentes evaluados. </p>"},{"location":"02.Tablero/Visualizaciones/01.Vision_General/#estructura-y-elementos-principales","title":"Estructura y Elementos Principales","text":"<ol> <li> <p>Men\u00fa Superior de Navegaci\u00f3n: </p> <ul> <li>Incluye secciones para navegar entre las distintas visualizaciones (Visi\u00f3n General Calidad de Datos, An\u00e1lisis de Fuentes de Datos y Componentes Calidad de Datos).  </li> <li>Permite al usuario cambiar f\u00e1cilmente de una perspectiva a otra sin salir de la vista principal.</li> </ul> </li> <li> <p>Filtros </p> <ul> <li>Ubicados superior, ayudan a refinar la informaci\u00f3n por base, esquema y tabla.</li> <li>Aportan interactividad, permitiendo que los gr\u00e1ficos y tablas se actualicen seg\u00fan las selecciones del usuario.</li> </ul> </li> <li> <p>Zona Central de Indicadores Generales Estrat\u00e9gicos:     Presenta m\u00e9tricas relevantes del proceso de calidad de datos, como:  </p> <ul> <li># Registros: N\u00famero de filas analizadas.  </li> <li># Bases de Datos: N\u00famero de bases de datos analizadas.  </li> <li># Esquemas: N\u00famero de esquemas analizados.  </li> <li># Tablas: N\u00famero de tablas analizadas.   </li> <li># Campos: N\u00famero de campos analizadas. </li> </ul> </li> <li> <p>Nivel General de Calidad: </p> <ul> <li>Indicador del nivel general de calidad de datos.  </li> <li>Representaci\u00f3n gr\u00e1fica del indicador.</li> <li>Matriz con los componentes de calidad evaluados junto con su descripci\u00f3n y acciones generales.</li> </ul> </li> <li> <p>Nivel de los Componentes Calidad de Datos </p> <ul> <li>Gr\u00e1fico de red con los niveles de los componentes de calidad de datos: Exactitud, Completitud, Consistencia, Actualizaci\u00f3n, Trazabilidad, Conformidad, Credibilidad y Compresibilidad.</li> <li>Nivel Actual en azul y nivel m\u00e1ximo en naranja.</li> <li>Fecha de medici\u00f3n de los indicadores.</li> </ul> </li> </ol>"},{"location":"02.Tablero/Visualizaciones/02.Analisis_Fuentes_Datos/","title":"An\u00e1lisis de Fuentes de Datos","text":""},{"location":"02.Tablero/Visualizaciones/02.Analisis_Fuentes_Datos/#analisis-fuentes-de-datos","title":"An\u00e1lisis Fuentes de Datos","text":"<p>El objetivo de esta visualizaci\u00f3n es presentar las caracter\u00edsticas de las bases de datos, sus metadatos e indicadores hist\u00f3ricos; para lo cual se presentan los siguientes aspectos a analizar: Distribuci\u00f3n de almacenamiento por registros y campos, relaciones entre las tablas para ver las llaves primarias y for\u00e1neas, metadatos de grupos y tablas, metadatos de campos y, el comportamiento hist\u00f3rico de los indicadores de calidad. </p>"},{"location":"02.Tablero/Visualizaciones/02.Analisis_Fuentes_Datos/#distribucion-de-almacenamiento","title":"Distribuci\u00f3n de Almacenamiento","text":""},{"location":"02.Tablero/Visualizaciones/02.Analisis_Fuentes_Datos/#objetivo","title":"Objetivo","text":"<p>Analizar el almacenamiento en las bases de datos medido en n\u00famero de registros y n\u00famero de columnas.</p>"},{"location":"02.Tablero/Visualizaciones/02.Analisis_Fuentes_Datos/#estructura-y-elementos-principales","title":"Estructura y Elementos Principales","text":"<ol> <li> <p>Men\u00fa Superior de Navegaci\u00f3n: </p> <ul> <li>Incluye secciones para navegar entre las distintas visualizaciones (Visi\u00f3n General Calidad de Datos, An\u00e1lisis de Fuentes de Datos y Componentes Calidad de Datos).  </li> <li>Permite al usuario cambiar f\u00e1cilmente de una perspectiva a otra sin salir de la vista principal.</li> </ul> </li> <li> <p>Filtros </p> <ul> <li>Ubicados superior, ayudan a refinar la informaci\u00f3n por base, esquema, tabla y campo.</li> <li>Aportan interactividad, permitiendo que los gr\u00e1ficos y tablas se actualicen seg\u00fan las selecciones del usuario.</li> </ul> </li> <li> <p>Zona Central de Indicadores Generales Estrat\u00e9gicos:     Presenta m\u00e9tricas relevantes como:  </p> <ul> <li># Registros: N\u00famero de filas analizadas.  </li> <li># Bases de Datos: N\u00famero de bases de datos analizadas.  </li> <li># Esquemas: N\u00famero de esquemas analizados.  </li> <li># Tablas: N\u00famero de tablas analizadas.   </li> <li># Campos: N\u00famero de campos analizadas. </li> <li>Nivel General Calidad: Nivel general de calidad de datos a la fecha de corte.</li> </ul> </li> <li> <p>Panel Lateral Izquierdo:     Contiene opciones para profundizar en los aspectos clave del an\u00e1lisis de fuentes, tales como:  </p> <ul> <li>Distribuci\u00f3n de Almacenamiento: Analiza el almacenamiento en las bases de datos medido en n\u00famero de registros y n\u00famero de columnas.</li> <li>Relaciones Tablas: Muestra el inventario de llaves for\u00e1neas relacionadas con los datos param\u00e9tricos.</li> <li>Metadatos Grupos y Tablas: Muestra informaci\u00f3n de los metadatos de los grupos y tablas analizados.</li> <li>Metadatos Campos: Muestra informaci\u00f3n de los metadatos de los campos analizados.</li> <li>Comportamientos Hist\u00f3ricos Indicadores Calidad: Presenta la informaci\u00f3n hist\u00f3rica de los indicadores de calidad. </li> </ul> </li> <li> <p>Distribuci\u00f3n de Almacenamiento </p> <ul> <li>Contiene un men\u00fa para seleccionar la opci\u00f3n de an\u00e1lisis por n\u00famero de registros, porcentaje de registros, n\u00famero de campos y porcentaje de campos.</li> <li>Diagrama jer\u00e1rquico que explica la opci\u00f3n seleccionada por base y tabla.</li> </ul> </li> </ol>"},{"location":"02.Tablero/Visualizaciones/02.Analisis_Fuentes_Datos/#relaciones-entre-tablas","title":"Relaciones entre Tablas","text":""},{"location":"02.Tablero/Visualizaciones/02.Analisis_Fuentes_Datos/#objetivo_1","title":"Objetivo","text":"<p>Presentar el inventario de llaves for\u00e1neas relacionadas con los datos param\u00e9tricos.</p>"},{"location":"02.Tablero/Visualizaciones/02.Analisis_Fuentes_Datos/#estructura-y-elementos-principales_1","title":"Estructura y Elementos Principales","text":"<ol> <li> <p>Men\u00fa Superior de Navegaci\u00f3n: </p> <ul> <li>Incluye secciones para navegar entre las distintas visualizaciones (Visi\u00f3n General Calidad de Datos, An\u00e1lisis de Fuentes de Datos y Componentes Calidad de Datos).  </li> <li>Permite al usuario cambiar f\u00e1cilmente de una perspectiva a otra sin salir de la vista principal.</li> </ul> </li> <li> <p>Filtros </p> <ul> <li>Ubicados superior, ayudan a refinar la informaci\u00f3n por base, esquema, tabla y campo.</li> <li>Aportan interactividad, permitiendo que los gr\u00e1ficos y tablas se actualicen seg\u00fan las selecciones del usuario.</li> </ul> </li> <li> <p>Zona Central de Indicadores Generales Estrat\u00e9gicos:     Presenta m\u00e9tricas relevantes como:  </p> <ul> <li># Registros: N\u00famero de filas analizadas.  </li> <li># Bases de Datos: N\u00famero de bases de datos analizadas.  </li> <li># Esquemas: N\u00famero de esquemas analizados.  </li> <li># Tablas: N\u00famero de tablas analizadas.   </li> <li># Campos: N\u00famero de campos analizadas. </li> <li>Nivel General Calidad: Nivel general de calidad de datos a la fecha de corte.</li> </ul> </li> <li> <p>Panel Lateral Izquierdo:     Contiene opciones para profundizar en los aspectos clave del an\u00e1lisis de fuentes, tales como:  </p> <ul> <li>Distribuci\u00f3n de Almacenamiento: Analiza el almacenamiento en las bases de datos medido en n\u00famero de registros y n\u00famero de columnas.</li> <li>Relaciones entre Tablas: Muestra el inventario de llaves for\u00e1neas relacionadas con los datos param\u00e9tricos.</li> <li>Metadatos Grupos y Tablas: Muestra informaci\u00f3n de los metadatos de los grupos y tablas analizados.</li> <li>Metadatos Campos: Muestra informaci\u00f3n de los metadatos de los campos analizados.</li> <li>Comportamientos Hist\u00f3ricos Indicadores Calidad: Presenta la informaci\u00f3n hist\u00f3rica de los indicadores de calidad. </li> </ul> </li> <li> <p>Relaciones entre Tablas </p> <ul> <li>Indicadores de n\u00famero de llaves primarias y for\u00e1neas.</li> <li>Indicadores de consistencia y exactitud.</li> <li>Matriz con informaci\u00f3n de la llave for\u00e1nea: Base, nombre de la llave for\u00e1neas, tabla origen, campo origen, tabla destino y campo destino.</li> </ul> </li> </ol>"},{"location":"02.Tablero/Visualizaciones/02.Analisis_Fuentes_Datos/#metadatos-grupos-y-tablas","title":"Metadatos Grupos y Tablas","text":""},{"location":"02.Tablero/Visualizaciones/02.Analisis_Fuentes_Datos/#objetivo_2","title":"Objetivo","text":"<p>Presentar la informaci\u00f3n asociadad a los metadatos de los grupos y tablas analizados.</p>"},{"location":"02.Tablero/Visualizaciones/02.Analisis_Fuentes_Datos/#estructura-y-elementos-principales_2","title":"Estructura y Elementos Principales","text":"<ol> <li> <p>Men\u00fa Superior de Navegaci\u00f3n: </p> <ul> <li>Incluye secciones para navegar entre las distintas visualizaciones (Visi\u00f3n General Calidad de Datos, An\u00e1lisis de Fuentes de Datos y Componentes Calidad de Datos).  </li> <li>Permite al usuario cambiar f\u00e1cilmente de una perspectiva a otra sin salir de la vista principal.</li> </ul> </li> <li> <p>Filtros </p> <ul> <li>Ubicados superior, ayudan a refinar la informaci\u00f3n por base, esquema, tabla y campo.</li> <li>Aportan interactividad, permitiendo que los gr\u00e1ficos y tablas se actualicen seg\u00fan las selecciones del usuario.</li> </ul> </li> <li> <p>Zona Central de Indicadores Generales Estrat\u00e9gicos:     Presenta m\u00e9tricas relevantes como:  </p> <ul> <li># Registros: N\u00famero de filas analizadas.  </li> <li># Bases de Datos: N\u00famero de bases de datos analizadas.  </li> <li># Esquemas: N\u00famero de esquemas analizados.  </li> <li># Tablas: N\u00famero de tablas analizadas.   </li> <li># Campos: N\u00famero de campos analizadas. </li> <li>Nivel General Calidad: Nivel general de calidad de datos a la fecha de corte.</li> </ul> </li> <li> <p>Panel Lateral Izquierdo:     Contiene opciones para profundizar en los aspectos clave del an\u00e1lisis de fuentes, tales como:  </p> <ul> <li>Distribuci\u00f3n de Almacenamiento: Analiza el almacenamiento en las bases de datos medido en n\u00famero de registros y n\u00famero de columnas.</li> <li>Relaciones entre Tablas: Muestra el inventario de llaves for\u00e1neas relacionadas con los datos param\u00e9tricos.</li> <li>Metadatos Grupos y Tablas: Muestra informaci\u00f3n de los metadatos de los grupos y tablas analizados.</li> <li>Metadatos Campos: Muestra informaci\u00f3n de los metadatos de los campos analizados.</li> <li>Comportamientos Hist\u00f3ricos Indicadores Calidad: Presenta la informaci\u00f3n hist\u00f3rica de los indicadores de calidad. </li> </ul> </li> <li> <p>Metadatos Grupos y Tablas </p> <ul> <li>Indicadores de conformidad y credibilidad</li> <li>Descripci\u00f3n de grupos y tablas por grupo.</li> <li>Gr\u00e1fico circular con distribuci\u00f3n de tablas por si cuentan con metadatos completos.</li> <li>Gr\u00e1fico circular con distribuci\u00f3n de tablas por si requieren actualizar registros.</li> </ul> </li> </ol>"},{"location":"02.Tablero/Visualizaciones/02.Analisis_Fuentes_Datos/#metadatos-campos","title":"Metadatos Campos","text":""},{"location":"02.Tablero/Visualizaciones/02.Analisis_Fuentes_Datos/#objetivo_3","title":"Objetivo","text":"<p>Presentar la informaci\u00f3n asociadad a los metadatos de los campos analizados.</p>"},{"location":"02.Tablero/Visualizaciones/02.Analisis_Fuentes_Datos/#estructura-y-elementos-principales_3","title":"Estructura y Elementos Principales","text":"<ol> <li> <p>Men\u00fa Superior de Navegaci\u00f3n: </p> <ul> <li>Incluye secciones para navegar entre las distintas visualizaciones (Visi\u00f3n General Calidad de Datos, An\u00e1lisis de Fuentes de Datos y Componentes Calidad de Datos).  </li> <li>Permite al usuario cambiar f\u00e1cilmente de una perspectiva a otra sin salir de la vista principal.</li> </ul> </li> <li> <p>Filtros </p> <ul> <li>Ubicados superior, ayudan a refinar la informaci\u00f3n por base, esquema, tabla y campo.</li> <li>Aportan interactividad, permitiendo que los gr\u00e1ficos y tablas se actualicen seg\u00fan las selecciones del usuario.</li> </ul> </li> <li> <p>Zona Central de Indicadores Generales Estrat\u00e9gicos:     Presenta m\u00e9tricas relevantes como:  </p> <ul> <li># Registros: N\u00famero de filas analizadas.  </li> <li># Bases de Datos: N\u00famero de bases de datos analizadas.  </li> <li># Esquemas: N\u00famero de esquemas analizados.  </li> <li># Tablas: N\u00famero de tablas analizadas.   </li> <li># Campos: N\u00famero de campos analizadas. </li> <li>Nivel General Calidad: Nivel general de calidad de datos a la fecha de corte.</li> </ul> </li> <li> <p>Panel Lateral Izquierdo:     Contiene opciones para profundizar en los aspectos clave del an\u00e1lisis de fuentes, tales como:  </p> <ul> <li>Distribuci\u00f3n de Almacenamiento: Analiza el almacenamiento en las bases de datos medido en n\u00famero de registros y n\u00famero de columnas.</li> <li>Relaciones entre Tablas: Muestra el inventario de llaves for\u00e1neas relacionadas con los datos param\u00e9tricos.</li> <li>Metadatos Grupos y Tablas: Muestra informaci\u00f3n de los metadatos de los grupos y tablas analizados.</li> <li>Metadatos Campos: Muestra informaci\u00f3n de los metadatos de los campos analizados.</li> <li>Comportamientos Hist\u00f3ricos Indicadores Calidad: Presenta la informaci\u00f3n hist\u00f3rica de los indicadores de calidad. </li> </ul> </li> <li> <p>Metadatos Campos </p> <ul> <li>Indicadores de conformidad y compresibilidad</li> <li>Gr\u00e1fico circular con distribuci\u00f3n de campos por si cuentan con metadatos completos.</li> <li>Inventario de campos con metadatos incompletos.</li> </ul> </li> </ol>"},{"location":"02.Tablero/Visualizaciones/02.Analisis_Fuentes_Datos/#comportamientos-historicos-indicadores-calidad","title":"Comportamientos Hist\u00f3ricos Indicadores Calidad","text":""},{"location":"02.Tablero/Visualizaciones/02.Analisis_Fuentes_Datos/#objetivo_4","title":"Objetivo","text":"<p>Presenta la informaci\u00f3n hist\u00f3rica de los indicadores de calidad.</p>"},{"location":"02.Tablero/Visualizaciones/02.Analisis_Fuentes_Datos/#estructura-y-elementos-principales_4","title":"Estructura y Elementos Principales","text":"<ol> <li> <p>Men\u00fa Superior de Navegaci\u00f3n: </p> <ul> <li>Incluye secciones para navegar entre las distintas visualizaciones (Visi\u00f3n General Calidad de Datos, An\u00e1lisis de Fuentes de Datos y Componentes Calidad de Datos).  </li> <li>Permite al usuario cambiar f\u00e1cilmente de una perspectiva a otra sin salir de la vista principal.</li> </ul> </li> <li> <p>Filtros </p> <ul> <li>Ubicados superior, ayudan a refinar la informaci\u00f3n por base, esquema, tabla y campo.</li> <li>Aportan interactividad, permitiendo que los gr\u00e1ficos y tablas se actualicen seg\u00fan las selecciones del usuario.</li> </ul> </li> <li> <p>Zona Central de Indicadores Generales Estrat\u00e9gicos:     Presenta m\u00e9tricas relevantes como:  </p> <ul> <li># Registros: N\u00famero de filas analizadas.  </li> <li># Bases de Datos: N\u00famero de bases de datos analizadas.  </li> <li># Esquemas: N\u00famero de esquemas analizados.  </li> <li># Tablas: N\u00famero de tablas analizadas.   </li> <li># Campos: N\u00famero de campos analizadas. </li> <li>Nivel General Calidad: Nivel general de calidad de datos a la fecha de corte.</li> </ul> </li> <li> <p>Panel Lateral Izquierdo:     Contiene opciones para profundizar en los aspectos clave del an\u00e1lisis de fuentes, tales como:  </p> <ul> <li>Distribuci\u00f3n de Almacenamiento: Analiza el almacenamiento en las bases de datos medido en n\u00famero de registros y n\u00famero de columnas.</li> <li>Relaciones entre Tablas: Muestra el inventario de llaves for\u00e1neas relacionadas con los datos param\u00e9tricos.</li> <li>Metadatos Grupos y Tablas: Muestra informaci\u00f3n de los metadatos de los grupos y tablas analizados.</li> <li>Metadatos Campos: Muestra informaci\u00f3n de los metadatos de los campos analizados.</li> <li>Comportamientos Hist\u00f3ricos Indicadores Calidad: Presenta la informaci\u00f3n hist\u00f3rica de los indicadores de calidad. </li> </ul> </li> <li> <p>Comportamientos Hist\u00f3ricos Indicadores Calidad </p> <ul> <li>Hist\u00f3rico de los indicadores de Calidad. </li> </ul> </li> </ol>"},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/","title":"Componentes Calidad de Datos","text":""},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#componentes-calidad-de-datos","title":"Componentes Calidad de Datos","text":"<p>El objetivo de esta visualizaci\u00f3n es presentar el detalle de los 8 componentes de la calidad de datos junto con su comportamiento a nivel operativo; esto con el fin de tener informaci\u00f3n necesaria para la toma de decisiones que contribuyan a mejorar la calidad de los datos param\u00e9tricos de la historia cl\u00ednica. </p>"},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#exactitud","title":"Exactitud","text":""},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#objetivo","title":"Objetivo","text":"<p>Analizar si los datos corresponden al tipo de datos definido para los datos param\u00e9tricos.</p>"},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#estructura-y-elementos-principales","title":"Estructura y Elementos Principales","text":"<ol> <li> <p>Men\u00fa Superior de Navegaci\u00f3n: </p> <ul> <li>Incluye secciones para navegar entre las distintas visualizaciones (Visi\u00f3n General Calidad de Datos, An\u00e1lisis de Fuentes de Datos y Componentes Calidad de Datos).  </li> <li>Permite al usuario cambiar f\u00e1cilmente de una perspectiva a otra sin salir de la vista principal.</li> </ul> </li> <li> <p>Filtros </p> <ul> <li>Ubicados superior, ayudan a refinar la informaci\u00f3n por base, esquema, tabla y campo.</li> <li>Aportan interactividad, permitiendo que los gr\u00e1ficos y tablas se actualicen seg\u00fan las selecciones del usuario.</li> </ul> </li> <li> <p>Zona Central de Indicadores Generales Estrat\u00e9gicos:     Presenta m\u00e9tricas relevantes como:  </p> <ul> <li># Registros: N\u00famero de filas analizadas.  </li> <li># Bases de Datos: N\u00famero de bases de datos analizadas.  </li> <li># Esquemas: N\u00famero de esquemas analizados.  </li> <li># Tablas: N\u00famero de tablas analizadas.   </li> <li># Campos: N\u00famero de campos analizadas. </li> <li>Nivel General Calidad: Nivel general de calidad de datos a la fecha de corte.</li> </ul> </li> <li> <p>Panel Lateral Izquierdo:     Contiene opciones para profundizar en cada componente de calidad de datos:</p> <ul> <li>Exactitud: Analiza si los datos corresponden al tipo de datos definido para los datos param\u00e9tricos.</li> <li>Completitud: Analiza el porcentaje de nulos o registros vac\u00edos en los datos param\u00e9tricos.</li> <li>Consistencia: Analiza si los datos param\u00e9tricos son coherentes y libres de contradicci\u00f3n.</li> <li>Actualizaci\u00f3n: Analiza la vigencia y actualidad de los datos param\u00e9tricos publicados.</li> <li>Trazabilidad: Analiza los metadatos asociados al manejo hist\u00f3rico de los datos param\u00e9tricos.</li> <li>Conformidad: Analiza el cumplimiento de los est\u00e1ndares para la descripci\u00f3n de metadatos. </li> <li>Credibilidad: Analiza si los datos param\u00e9tricos cuentan con fuentes confiables para los usuarios.</li> <li>Compresibilidad: Analiza si las caracter\u00edsticas de la informaci\u00f3n permiten al usuario leer e interpretar los datos.</li> </ul> </li> <li> <p>Exactitud </p> <ul> <li>Hist\u00f3rico de los indicador.</li> <li>Valor del indicador.</li> <li>N\u00famero de registros por campo y gr\u00e1fico circular con su distribuci\u00f3n por si cumplen con el tipo de dato definido.</li> <li>Gr\u00e1fico de barras con la distribuci\u00f3n de campos por cuantil del indicador</li> <li>Gr\u00e1fico de barras Distribuci\u00f3n de tablas por cuantil del indicador.</li> <li>Matriz con el top de campos con menor exactitud.</li> <li>Matriz con el top de tablas con menor exactitud</li> </ul> </li> </ol>"},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#completitud","title":"Completitud","text":""},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#objetivo_1","title":"Objetivo","text":"<p>Analizar el porcentaje de nulos o registros vac\u00edos en los datos param\u00e9tricos.</p>"},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#estructura-y-elementos-principales_1","title":"Estructura y Elementos Principales","text":"<ol> <li> <p>Men\u00fa Superior de Navegaci\u00f3n: </p> <ul> <li>Incluye secciones para navegar entre las distintas visualizaciones (Visi\u00f3n General Calidad de Datos, An\u00e1lisis de Fuentes de Datos y Componentes Calidad de Datos).  </li> <li>Permite al usuario cambiar f\u00e1cilmente de una perspectiva a otra sin salir de la vista principal.</li> </ul> </li> <li> <p>Filtros </p> <ul> <li>Ubicados superior, ayudan a refinar la informaci\u00f3n por base, esquema, tabla y campo.</li> <li>Aportan interactividad, permitiendo que los gr\u00e1ficos y tablas se actualicen seg\u00fan las selecciones del usuario.</li> </ul> </li> <li> <p>Zona Central de Indicadores Generales Estrat\u00e9gicos:     Presenta m\u00e9tricas relevantes como:  </p> <ul> <li># Registros: N\u00famero de filas analizadas.  </li> <li># Bases de Datos: N\u00famero de bases de datos analizadas.  </li> <li># Esquemas: N\u00famero de esquemas analizados.  </li> <li># Tablas: N\u00famero de tablas analizadas.   </li> <li># Campos: N\u00famero de campos analizadas. </li> <li>Nivel General Calidad: Nivel general de calidad de datos a la fecha de corte.</li> </ul> </li> <li> <p>Panel Lateral Izquierdo:     Contiene opciones para profundizar en cada componente de calidad de datos:</p> <ul> <li>Exactitud: Analiza si los datos corresponden al tipo de datos definido para los datos param\u00e9tricos.</li> <li>Completitud: Analiza el porcentaje de nulos o registros vac\u00edos en los datos param\u00e9tricos.</li> <li>Consistencia: Analiza si los datos param\u00e9tricos son coherentes y libres de contradicci\u00f3n.</li> <li>Actualizaci\u00f3n: Analiza la vigencia y actualidad de los datos param\u00e9tricos publicados.</li> <li>Trazabilidad: Analiza los metadatos asociados al manejo hist\u00f3rico de los datos param\u00e9tricos.</li> <li>Conformidad: Analiza el cumplimiento de los est\u00e1ndares para la descripci\u00f3n de metadatos.</li> <li>Credibilidad: Analiza si los datos param\u00e9tricos cuentan con fuentes confiables para los usuarios.</li> <li>Compresibilidad: Analiza si las caracter\u00edsticas de la informaci\u00f3n permiten al usuario leer e interpretar los datos.</li> </ul> </li> <li> <p>Completitud </p> <ul> <li>Hist\u00f3rico de los indicador.</li> <li>Valor del indicador.</li> <li>N\u00famero de registros por campo y gr\u00e1fico circular con su distribuci\u00f3n por nulos y no nulos.</li> <li>Gr\u00e1fico de barras con la distribuci\u00f3n de campos por cuantil del indicador.</li> <li>Gr\u00e1fico de barras Distribuci\u00f3n de tablas por cuantil del indicador.</li> <li>Matriz con el top de campos con menor completitud.</li> <li>Matriz con el top de tablas con menor completitud.</li> </ul> </li> </ol>"},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#consistencia","title":"Consistencia","text":""},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#objetivo_2","title":"Objetivo","text":"<p>Analizar si los datos param\u00e9tricos son coherentes y libres de contradicci\u00f3n.</p>"},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#estructura-y-elementos-principales_2","title":"Estructura y Elementos Principales","text":"<ol> <li> <p>Men\u00fa Superior de Navegaci\u00f3n: </p> <ul> <li>Incluye secciones para navegar entre las distintas visualizaciones (Visi\u00f3n General Calidad de Datos, An\u00e1lisis de Fuentes de Datos y Componentes Calidad de Datos).  </li> <li>Permite al usuario cambiar f\u00e1cilmente de una perspectiva a otra sin salir de la vista principal.</li> </ul> </li> <li> <p>Filtros </p> <ul> <li>Ubicados superior, ayudan a refinar la informaci\u00f3n por base, esquema, tabla y campo.</li> <li>Aportan interactividad, permitiendo que los gr\u00e1ficos y tablas se actualicen seg\u00fan las selecciones del usuario.</li> </ul> </li> <li> <p>Zona Central de Indicadores Generales Estrat\u00e9gicos:     Presenta m\u00e9tricas relevantes como:  </p> <ul> <li># Registros: N\u00famero de filas analizadas.  </li> <li># Bases de Datos: N\u00famero de bases de datos analizadas.  </li> <li># Esquemas: N\u00famero de esquemas analizados.  </li> <li># Tablas: N\u00famero de tablas analizadas.   </li> <li># Campos: N\u00famero de campos analizadas. </li> <li>Nivel General Calidad: Nivel general de calidad de datos a la fecha de corte.</li> </ul> </li> <li> <p>Panel Lateral Izquierdo:     Contiene opciones para profundizar en cada componente de calidad de datos:</p> <ul> <li>Exactitud: Analiza si los datos corresponden al tipo de datos definido para los datos param\u00e9tricos.</li> <li>Completitud: Analiza el porcentaje de nulos o registros vac\u00edos en los datos param\u00e9tricos.</li> <li>Consistencia: Analiza si los datos param\u00e9tricos son coherentes y libres de contradicci\u00f3n.</li> <li>Actualizaci\u00f3n: Analiza la vigencia y actualidad de los datos param\u00e9tricos publicados.</li> <li>Trazabilidad: Analiza los metadatos asociados al manejo hist\u00f3rico de los datos param\u00e9tricos.</li> <li>Conformidad: Analiza el cumplimiento de los est\u00e1ndares para la descripci\u00f3n de metadatos.</li> <li>Credibilidad: Analiza si los datos param\u00e9tricos cuentan con fuentes confiables para los usuarios.</li> <li>Compresibilidad: Analiza si las caracter\u00edsticas de la informaci\u00f3n permiten al usuario leer e interpretar los datos.</li> </ul> </li> <li> <p>Consistencia </p> <ul> <li>Hist\u00f3rico de los indicador.</li> <li>Valor del indicador.</li> <li>Gr\u00e1fico circular con distribuci\u00f3n de registros por si son duplicados o no.</li> <li>Gr\u00e1fico circular con distribuci\u00f3n de campos por si su nombre est\u00e1 repetido o no.</li> <li>Gr\u00e1fico de barras Distribuci\u00f3n de tablas por cuantil del indicador.</li> <li>Matriz con el top de campos con nombre repetido.</li> <li>Matriz con el top de tablas con menor consistencia.</li> </ul> </li> </ol>"},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#actualizacion","title":"Actualizaci\u00f3n","text":""},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#objetivo_3","title":"Objetivo","text":"<p>Analizar la vigencia y actualidad de los datos param\u00e9tricos publicados.</p>"},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#estructura-y-elementos-principales_3","title":"Estructura y Elementos Principales","text":"<ol> <li> <p>Men\u00fa Superior de Navegaci\u00f3n: </p> <ul> <li>Incluye secciones para navegar entre las distintas visualizaciones (Visi\u00f3n General Calidad de Datos, An\u00e1lisis de Fuentes de Datos y Componentes Calidad de Datos).  </li> <li>Permite al usuario cambiar f\u00e1cilmente de una perspectiva a otra sin salir de la vista principal.</li> </ul> </li> <li> <p>Filtros </p> <ul> <li>Ubicados superior, ayudan a refinar la informaci\u00f3n por base, esquema, tabla y campo.</li> <li>Aportan interactividad, permitiendo que los gr\u00e1ficos y tablas se actualicen seg\u00fan las selecciones del usuario.</li> </ul> </li> <li> <p>Zona Central de Indicadores Generales Estrat\u00e9gicos:     Presenta m\u00e9tricas relevantes como:  </p> <ul> <li># Registros: N\u00famero de filas analizadas.  </li> <li># Bases de Datos: N\u00famero de bases de datos analizadas.  </li> <li># Esquemas: N\u00famero de esquemas analizados.  </li> <li># Tablas: N\u00famero de tablas analizadas.   </li> <li># Campos: N\u00famero de campos analizadas. </li> <li>Nivel General Calidad: Nivel general de calidad de datos a la fecha de corte.</li> </ul> </li> <li> <p>Panel Lateral Izquierdo:     Contiene opciones para profundizar en cada componente de calidad de datos:</p> <ul> <li>Exactitud: Analiza si los datos corresponden al tipo de datos definido para los datos param\u00e9tricos.</li> <li>Completitud: Analiza el porcentaje de nulos o registros vac\u00edos en los datos param\u00e9tricos.</li> <li>Consistencia: Analiza si los datos param\u00e9tricos son coherentes y libres de contradicci\u00f3n.</li> <li>Actualizaci\u00f3n: Analiza la vigencia y actualidad de los datos param\u00e9tricos publicados.</li> <li>Trazabilidad: Analiza los metadatos asociados al manejo hist\u00f3rico de los datos param\u00e9tricos.</li> <li>Conformidad: Analiza el cumplimiento de los est\u00e1ndares para la descripci\u00f3n de metadatos.</li> <li>Credibilidad: Analiza si los datos param\u00e9tricos cuentan con fuentes confiables para los usuarios.</li> <li>Compresibilidad: Analiza si las caracter\u00edsticas de la informaci\u00f3n permiten al usuario leer e interpretar los datos.</li> </ul> </li> <li> <p>Actualizaci\u00f3n </p> <ul> <li>Hist\u00f3rico de los indicador.</li> <li>Valor del indicador.</li> <li>Gr\u00e1fico de barras con distribuci\u00f3n de tablas por cuantil del indicador.</li> <li>Matriz con tablas que requieren actualizar registros.</li> <li>Gr\u00e1fico circular con distribuci\u00f3n de tablas por si requieren actualizar registros.</li> </ul> </li> </ol>"},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#trazabilidad","title":"Trazabilidad","text":""},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#objetivo_4","title":"Objetivo","text":"<p>Analizar los metadatos asociados al manejo hist\u00f3rico de los datos param\u00e9tricos.</p>"},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#estructura-y-elementos-principales_4","title":"Estructura y Elementos Principales","text":"<ol> <li> <p>Men\u00fa Superior de Navegaci\u00f3n: </p> <ul> <li>Incluye secciones para navegar entre las distintas visualizaciones (Visi\u00f3n General Calidad de Datos, An\u00e1lisis de Fuentes de Datos y Componentes Calidad de Datos).  </li> <li>Permite al usuario cambiar f\u00e1cilmente de una perspectiva a otra sin salir de la vista principal.</li> </ul> </li> <li> <p>Filtros </p> <ul> <li>Ubicados superior, ayudan a refinar la informaci\u00f3n por base, esquema, tabla y campo.</li> <li>Aportan interactividad, permitiendo que los gr\u00e1ficos y tablas se actualicen seg\u00fan las selecciones del usuario.</li> </ul> </li> <li> <p>Zona Central de Indicadores Generales Estrat\u00e9gicos:     Presenta m\u00e9tricas relevantes como:  </p> <ul> <li># Registros: N\u00famero de filas analizadas.  </li> <li># Bases de Datos: N\u00famero de bases de datos analizadas.  </li> <li># Esquemas: N\u00famero de esquemas analizados.  </li> <li># Tablas: N\u00famero de tablas analizadas.   </li> <li># Campos: N\u00famero de campos analizadas. </li> <li>Nivel General Calidad: Nivel general de calidad de datos a la fecha de corte.</li> </ul> </li> <li> <p>Panel Lateral Izquierdo:     Contiene opciones para profundizar en cada componente de calidad de datos:</p> <ul> <li>Exactitud: Analiza si los datos corresponden al tipo de datos definido para los datos param\u00e9tricos.</li> <li>Completitud: Analiza el porcentaje de nulos o registros vac\u00edos en los datos param\u00e9tricos.</li> <li>Consistencia: Analiza si los datos param\u00e9tricos son coherentes y libres de contradicci\u00f3n.</li> <li>Actualizaci\u00f3n: Analiza la vigencia y actualidad de los datos param\u00e9tricos publicados.</li> <li>Trazabilidad: Analiza los metadatos asociados al manejo hist\u00f3rico de los datos param\u00e9tricos.</li> <li>Conformidad: Analiza el cumplimiento de los est\u00e1ndares para la descripci\u00f3n de metadatos.</li> <li>Credibilidad: Analiza si los datos param\u00e9tricos cuentan con fuentes confiables para los usuarios.</li> <li>Compresibilidad: Analiza si las caracter\u00edsticas de la informaci\u00f3n permiten al usuario leer e interpretar los datos.</li> </ul> </li> <li> <p>Trazabilidad </p> <ul> <li>Hist\u00f3rico de los indicador.</li> <li>Valor del indicador.</li> <li>Gr\u00e1fico de barras con distribuci\u00f3n de tablas por cuantil del indicador.</li> <li>Gr\u00e1fico circular con distribuci\u00f3n de tablas por si requieren actualizar registros.</li> <li>Gr\u00e1fico circular con distribuci\u00f3n de tablas por si tienen fecha de actualizaci\u00f3n de metadatos.</li> <li>Gr\u00e1fico circular con distribuci\u00f3n de tablas por si tienen fecha de creaci\u00f3n.</li> <li>Matriz con tablas que requieren actualizar metadatos.</li> </ul> </li> </ol>"},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#conformidad","title":"Conformidad","text":""},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#objetivo_5","title":"Objetivo","text":"<p>Analizar el cumplimiento de los est\u00e1ndares para la descripci\u00f3n de metadatos.</p>"},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#estructura-y-elementos-principales_5","title":"Estructura y Elementos Principales","text":"<ol> <li> <p>Men\u00fa Superior de Navegaci\u00f3n: </p> <ul> <li>Incluye secciones para navegar entre las distintas visualizaciones (Visi\u00f3n General Calidad de Datos, An\u00e1lisis de Fuentes de Datos y Componentes Calidad de Datos).  </li> <li>Permite al usuario cambiar f\u00e1cilmente de una perspectiva a otra sin salir de la vista principal.</li> </ul> </li> <li> <p>Filtros </p> <ul> <li>Ubicados superior, ayudan a refinar la informaci\u00f3n por base, esquema, tabla y campo.</li> <li>Aportan interactividad, permitiendo que los gr\u00e1ficos y tablas se actualicen seg\u00fan las selecciones del usuario.</li> </ul> </li> <li> <p>Zona Central de Indicadores Generales Estrat\u00e9gicos:     Presenta m\u00e9tricas relevantes como:  </p> <ul> <li># Registros: N\u00famero de filas analizadas.  </li> <li># Bases de Datos: N\u00famero de bases de datos analizadas.  </li> <li># Esquemas: N\u00famero de esquemas analizados.  </li> <li># Tablas: N\u00famero de tablas analizadas.   </li> <li># Campos: N\u00famero de campos analizadas. </li> <li>Nivel General Calidad: Nivel general de calidad de datos a la fecha de corte.</li> </ul> </li> <li> <p>Panel Lateral Izquierdo:     Contiene opciones para profundizar en cada componente de calidad de datos:</p> <ul> <li>Exactitud: Analiza si los datos corresponden al tipo de datos definido para los datos param\u00e9tricos.</li> <li>Completitud: Analiza el porcentaje de nulos o registros vac\u00edos en los datos param\u00e9tricos.</li> <li>Consistencia: Analiza si los datos param\u00e9tricos son coherentes y libres de contradicci\u00f3n.</li> <li>Actualizaci\u00f3n: Analiza la vigencia y actualidad de los datos param\u00e9tricos publicados.</li> <li>Trazabilidad: Analiza los metadatos asociados al manejo hist\u00f3rico de los datos param\u00e9tricos.</li> <li>Conformidad: Analiza el cumplimiento de los est\u00e1ndares para la descripci\u00f3n de metadatos.</li> <li>Credibilidad: Analiza si los datos param\u00e9tricos cuentan con fuentes confiables para los usuarios.</li> <li>Compresibilidad: Analiza si las caracter\u00edsticas de la informaci\u00f3n permiten al usuario leer e interpretar los datos.</li> </ul> </li> <li> <p>Trazabilidad </p> <ul> <li>Hist\u00f3rico de los indicador.</li> <li>Valor del indicador.</li> <li>Gr\u00e1fico circular con distribuci\u00f3n de campos por si tienen metadatos completos.</li> <li>Gr\u00e1fico de barras con la distribuci\u00f3n de campos por cuantil del indicador</li> <li>Gr\u00e1fico de barras con distribuci\u00f3n de tablas por cuantil del indicador.</li> <li>Matriz con tablas que requieren actualizar metadatos.</li> </ul> </li> </ol>"},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#credibilidad","title":"Credibilidad","text":""},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#objetivo_6","title":"Objetivo","text":"<p>Analizar si los datos param\u00e9tricos cuentan con fuentes confiables para los usuarios.</p>"},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#estructura-y-elementos-principales_6","title":"Estructura y Elementos Principales","text":"<ol> <li> <p>Men\u00fa Superior de Navegaci\u00f3n: </p> <ul> <li>Incluye secciones para navegar entre las distintas visualizaciones (Visi\u00f3n General Calidad de Datos, An\u00e1lisis de Fuentes de Datos y Componentes Calidad de Datos).  </li> <li>Permite al usuario cambiar f\u00e1cilmente de una perspectiva a otra sin salir de la vista principal.</li> </ul> </li> <li> <p>Filtros </p> <ul> <li>Ubicados superior, ayudan a refinar la informaci\u00f3n por base, esquema, tabla y campo.</li> <li>Aportan interactividad, permitiendo que los gr\u00e1ficos y tablas se actualicen seg\u00fan las selecciones del usuario.</li> </ul> </li> <li> <p>Zona Central de Indicadores Generales Estrat\u00e9gicos:     Presenta m\u00e9tricas relevantes como:  </p> <ul> <li># Registros: N\u00famero de filas analizadas.  </li> <li># Bases de Datos: N\u00famero de bases de datos analizadas.  </li> <li># Esquemas: N\u00famero de esquemas analizados.  </li> <li># Tablas: N\u00famero de tablas analizadas.   </li> <li># Campos: N\u00famero de campos analizadas. </li> <li>Nivel General Calidad: Nivel general de calidad de datos a la fecha de corte.</li> </ul> </li> <li> <p>Panel Lateral Izquierdo:     Contiene opciones para profundizar en cada componente de calidad de datos:</p> <ul> <li>Exactitud: Analiza si los datos corresponden al tipo de datos definido para los datos param\u00e9tricos.</li> <li>Completitud: Analiza el porcentaje de nulos o registros vac\u00edos en los datos param\u00e9tricos.</li> <li>Consistencia: Analiza si los datos param\u00e9tricos son coherentes y libres de contradicci\u00f3n.</li> <li>Actualizaci\u00f3n: Analiza la vigencia y actualidad de los datos param\u00e9tricos publicados.</li> <li>Trazabilidad: Analiza los metadatos asociados al manejo hist\u00f3rico de los datos param\u00e9tricos.</li> <li>Conformidad: Analiza el cumplimiento de los est\u00e1ndares para la descripci\u00f3n de metadatos.</li> <li>Credibilidad: Analiza si los datos param\u00e9tricos cuentan con fuentes confiables para los usuarios.</li> <li>Compresibilidad: Analiza si las caracter\u00edsticas de la informaci\u00f3n permiten al usuario leer e interpretar los datos.</li> </ul> </li> <li> <p>Credibilidad </p> <ul> <li>Hist\u00f3rico de los indicador.</li> <li>Valor del indicador.</li> <li>Gr\u00e1fico de barras con distribuci\u00f3n de tablas por cuantil del indicador.</li> <li>Gr\u00e1fico circular con distribuci\u00f3n de tablas por si tienen nombre del propietario de los datos.</li> <li>Gr\u00e1fico circular con distribuci\u00f3n de tablas por si tienen datos de contacto del propietario de los datos.</li> <li>Matriz con tablas no tienen datos del propietario de los datos.</li> </ul> </li> </ol>"},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#compresibilidad","title":"Compresibilidad","text":""},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#objetivo_7","title":"Objetivo","text":"<p>Analizar si las caracter\u00edsticas de la informaci\u00f3n permiten al usuario leer e interpretar los datos.</p>"},{"location":"02.Tablero/Visualizaciones/03.Componentes_Calidad_Datos/#estructura-y-elementos-principales_7","title":"Estructura y Elementos Principales","text":"<ol> <li> <p>Men\u00fa Superior de Navegaci\u00f3n: </p> <ul> <li>Incluye secciones para navegar entre las distintas visualizaciones (Visi\u00f3n General Calidad de Datos, An\u00e1lisis de Fuentes de Datos y Componentes Calidad de Datos).  </li> <li>Permite al usuario cambiar f\u00e1cilmente de una perspectiva a otra sin salir de la vista principal.</li> </ul> </li> <li> <p>Filtros </p> <ul> <li>Ubicados superior, ayudan a refinar la informaci\u00f3n por base, esquema, tabla y campo.</li> <li>Aportan interactividad, permitiendo que los gr\u00e1ficos y tablas se actualicen seg\u00fan las selecciones del usuario.</li> </ul> </li> <li> <p>Zona Central de Indicadores Generales Estrat\u00e9gicos:     Presenta m\u00e9tricas relevantes como:  </p> <ul> <li># Registros: N\u00famero de filas analizadas.  </li> <li># Bases de Datos: N\u00famero de bases de datos analizadas.  </li> <li># Esquemas: N\u00famero de esquemas analizados.  </li> <li># Tablas: N\u00famero de tablas analizadas.   </li> <li># Campos: N\u00famero de campos analizadas. </li> <li>Nivel General Calidad: Nivel general de calidad de datos a la fecha de corte.</li> </ul> </li> <li> <p>Panel Lateral Izquierdo:     Contiene opciones para profundizar en cada componente de calidad de datos:</p> <ul> <li>Exactitud: Analiza si los datos corresponden al tipo de datos definido para los datos param\u00e9tricos.</li> <li>Completitud: Analiza el porcentaje de nulos o registros vac\u00edos en los datos param\u00e9tricos.</li> <li>Consistencia: Analiza si los datos param\u00e9tricos son coherentes y libres de contradicci\u00f3n.</li> <li>Actualizaci\u00f3n: Analiza la vigencia y actualidad de los datos param\u00e9tricos publicados.</li> <li>Trazabilidad: Analiza los metadatos asociados al manejo hist\u00f3rico de los datos param\u00e9tricos.</li> <li>Conformidad: Analiza el cumplimiento de los est\u00e1ndares para la descripci\u00f3n de metadatos.</li> <li>Credibilidad: Analiza si los datos param\u00e9tricos cuentan con fuentes confiables para los usuarios.</li> <li>Compresibilidad: Analiza si las caracter\u00edsticas de la informaci\u00f3n permiten al usuario leer e interpretar los datos.</li> </ul> </li> <li> <p>Compresibilidad </p> <ul> <li>Hist\u00f3rico de los indicador.</li> <li>Valor del indicador.</li> <li>Gr\u00e1fico de barras con distribuci\u00f3n de tablas por cuantil del indicador.</li> <li>Gr\u00e1fico circular con distribuci\u00f3n de campos por si tienen descripci\u00f3n.</li> <li>Matriz con campos que no tienen descripci\u00f3n.</li> </ul> </li> </ol>"}]}